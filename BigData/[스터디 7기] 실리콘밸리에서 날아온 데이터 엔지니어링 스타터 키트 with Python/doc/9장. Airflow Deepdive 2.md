# Table of Contents

- [1. 5주차 숙제와 리뷰](#1-5주차-숙제와-리뷰)
- [2. MySQL 테이블 복사하기 (production db (MySQL) -> DW (Redshift))](#2-mysql-테이블-복사하기-production-db-mysql---dw-redshift)
- [3. How to Backfill in Airflow](#3-how-to-backfill-in-airflow)
- [4. Summary Table 구현](#4-summary-table-구현)

---

데이터 인프라를 setup하면 가장 중요한 DAG는 Production DB에 있는 테이블들을 DW로 옮겨오는 것임.

# 1. 5주차 숙제와 리뷰

## A. Try/Except 사용시 유의할 점

```python

try:
	cur.execute(sql)
	cur.execute("COMMIT;")
except Exception as e:
	cur.execute("ROLLBACK;")
	raise
	
```

- except에서 `raise`를 호출하면 발생한 원래 exception이 위로 전파됨. (에러를 Airflow한테 알려주게 됨)
	- ETL을 관리하는 입장에서 어떤 에러가 감춰지는 것보다는 명확하게 드러나는 것이 더 좋음

## B. PostgresHook의 autocommit 파라미터

- Default 값은 False
	- 이 경우 BEGIN은 아무런 영향이 없음 (no-operation)
	- 모든 작업이 트랜잭션이라 COMMIT을 호출해야 COMMIT 됨

## C. DAG에서 task를 어느 정도로 분리하는 것이 좋을까?

- task를 많이 만들면 전체 DAG이 실행되는데 오래 걸리고 스케줄러에 부하가 감
	- task의 수가 Airflow 전체 performance에 영향을 많이 줌
	- task를 너무 잘게 분해하면 오히려 안좋음.
- task를 너무 적게 만들면 모듈화가 안되고 실패시 재실행 시간이 오래 걸림
- 오래 걸리는 DAG는 실패시 재실행이 쉽게 다수의 task로 나누는 것이 좋음

---

- 중간에 Fail을 했을 때 처음부터 다시 시작할 필요가 없게끔 task 수를 나누는 것이 좋음
	- 단, 너무 잘게 쪼개는 것은 비추천.
- DAG 수가 늘어나면, task 수가 늘어나면, 스케줄 되는 것만으로도 시간이 꽤 걸림.

## D. Weather_forecast DAG 구현해보기

### a. max_active_runs

```python

dag = DAG(
	dag_id = 'Weather_to_Redshift',
	start_date = datetime(2021,12,10), # 날짜가 미래인 경우 실행이 안됨
	schedule_interval = '0 2 * * *', # 적당히 조절
	max_active_runs = 1,
	catchup = False,
	default_args = {
		'retries': 1,
		'retry_delay': timedelta(minutes=3),
	}
)

```

- max_active_runs = 1
	- 이 DAG는 한번에 하나만 돔(실행됨)
	- 1년치를 Back fill 할 때, 365개의 DAG가 Sequential하게 하나씩 돌아감.
- max_active_runs = 365
	- 365일치의 데이터를 Back fill하기 위해서 365개의 데이터가 동시에 돌면서 각자 다른 날짜를 처리함.
	- 하지만, 리소스가 그만큼 있어야 함.
	- 즉, 이 파라미터의 upper bound는 해당 서버의 CPU의 개수에 의해 결정됨.

### b. CTAS

#### ㄱ) CTAS의 문제점

- 타입이 조금씩 달라질 수 있음
- default와 같은 attribute가 보존되지 않음

#### ㄴ) CTAS 문제 해결 방법

- 매번 똑같이 `CRATE TABLE`을 한다.
- `CREATE TABLE (LIKE)`을 한다.
	- LIKE를 사용할 때, `INCLUDING DEFAULTS`를 붙여줘야 함.
		- INCLUDING DEFAULTS를 통해 DEFAULT Attribute가 동일해짐.

## E. Airflow 환경 설정

### a. 환경 설정 파일이 수정되었다면, 이를 실제로 반영하기 위해서 해야 하는 일은?

```
sudo systemctl restart airflow-webserver
sudo systemctl restart airflow-scheduler
```

- airflow db init을 하면 airflow가 reset되버림.
	- DAG가 등록된 내용, 과거에 DAG가 돌았던 기록 등이 사라짐

## F. dags 폴더에서 코딩시 작성한다면 주의할 점

- Airflow는 dags 폴더를 주기적으로 스캔함
	- dags 폴더에 있는 모든 파이썬 파일들을 스캔하는데, `from airflow import DAG`가 import된 파일들만 더 구체적으로 스캔한다.

```
[core]
dags_folder = /var/lib/airflow/dags
# How often (in seconds) to scan the DAGs directory for new files. Default to 5 minutes.
dag_dir_list_interval = 300
```

- 이때 DAG 모듈이 들어있는 모든 파일들의 메인 함수가 실행이 됨
	- 이 경우 본의 아니게 개발중인 테스트 코드도 실행될 수 있음

---

- DAG에 해당하는 파이썬 코드에는 DAG를 생성하는 코드와 task를 생성하는 코드 이외에는 들어가면 안됨.

## G. DAG 디버깅하기 혹은 에러 메세지 확인하기

### a. Web UI를 이용하는 방법

- Airflow WebUI DAG UI에서 문제가 있는 DAG를 클릭하고 거기서 빨간색으로 표시된 task 클릭 후 View Log로 확인

![](/bin/DE7_image/DE7_9_1.png)

![](/bin/DE7_image/DE7_9_2.png)

- 녹색
	- 잘 돌아간 것
- 빨간색
	- 에러가 생긴 것
- 노란색
	- 앞에 것이 Fail하여 아직 실행될 기회를 잡지 못한 task

![](/bin/DE7_image/DE7_9_3.png)

### b. Airflow 서버에서 확인하는 방법

- Airflow 서버에서 airflow 계정으로 아래 명령 실행

```
airflow dags list
airflow tasks list dag_id
airflow tasks test dag_id task_id execution_date # 2021-09-11
	- airflow tasks run dag_id task_id execution_date # 2021-09-11
```

- test
	- airflow metadata에 기록이 안됨
- run
	- airflow metadata에 dag의 task를 실행한 것이 기록됨

# 2. MySQL 테이블 복사하기 (production db (MySQL) -> DW (Redshift))

## A. 구현하려는 ETL

![](/bin/DE7_image/DE7_9_4.png)

![](/bin/DE7_image/DE7_9_5.png)

1. Airflow가 복사해올 Table을 인지하고 MySQL에서 local file로 다운로드 받음
2. 다운로드 받은 file을 S3로 업로드 (S3의 특정 bucket으로 로딩)
3. Redshift의 COPY 명령어를 사용하여 원하는 Redshift의 table로 Bulk 적재함.

## B. Connection object 확인

- Redshift Connection 설정
- MySQL Connection 설정 (Production Database)
- S3 Connection 설정

> 만약 S3가 안보인다면, `sudo pip3 install apache-airflow-providers-amazon==2.1.0`를 실행하면 됨.

## C. 테이블 리뷰

### a. MySQL의 테이블 리뷰 (OLTP, Production Database)

```sql

CREATE TABLE prod.nps(
	id INT NOT NULL AUTO_INCREMENT primary key,
	created_at timestamp,
	score smallint
);

```

이미 테이블은 MySQL에 만들어져 있고 레코드들이 존재하며, 이를 Redshift로 COPY할 예정

### b. Redshift (OLAP, Data Warehouse)에 해당 테이블 생성

```sql

CREATE TABLE (본인의 스키마).nps(
	id INT NOT NULL primary key,
	created_at timestamp,
	score smallint
);

```

이 테이블들은 Redshift쪽에 본인 스키마 밑에 별도로 만들고 뒤에서 실습할 DAG를 통해 MySQL쪽 테이블로부터 Redshift테이블로 복사하는 것이 우리가 할 실습.

## D. MySQL_to_Redshift DAG의 Task 구성

- MySQLToS3Operator
	- MySQL SQL 결과 -> S3
		- (s3://{top_level_directory}/{본인 ID}-nps)
	- s3://s3_bucket/s3_key
- S3ToRedshiftOperator
	- S3 -> Redshift 테이블
		- bulk insert해주는 task
		- s3://{top_level_directory}/{본인 ID}-nps) -> Redshift (본인 스키마.nps)
		- COPY command is used

## E. Bulk Update Sequence - COPY SQL

![](/bin/DE7_image/DE7_9_6.png)

- production db의 내용을 읽어서 redshift에 적재를 해주는 job이 주기적으로 실행된다.
	- streaming이 아닌 batch job이기 때문.

---

- batch job
	- 1시간에 한번 / 하루에 한번 등 production db에 있는 table을 주기적으로 읽어다가 S3에 업로드하고 COPY command 날려서 Redshift의 특정 테이블로 적재하는 일을 한다.

---

- 만약, record의 수가 적다면. (100개, 1000개 등)
	- S3에 로딩할 필요없이 production db에서 읽어서 redshift에 INSERT하는게 훨씬 빠름
	- 매번 full refresh하면 됨.
- record의 수가 많다면, (billion이 넘는 테이블)
	- record의 규모가 커서 매번 full refresh를 할 수 없음.
		- 이정도 규모면 테이블의 record가 잘 변하지도 않음
	- Incremental update를 해야함
	- source가 되는 production database가 primary key를 기본적으로 유지해주기 때문에 중복처리를 할 필요가 없음

## F. 코드 리뷰

### a. MySQL_to_Redshift (Full Refresh)

- 2개의 Operator를 사용해서 구현
	- MySQLToS3Operator
	- S3ToRedshiftOperator
- MySQL에 있는 테이블 nps를 Redshift 내의 스키마 밑의 nps 테이블로 복사
	- S3를 경우해서 COPY 명령으로 복사

---

```python

mysql_to_s3_nps = MySQLToS3Operator(
	task_id = 'mysql_to_s3_nps',
	query = "SELECT * FROM prod.nps",
	s3_bucket = s3_bucket,
	s3_key = s3_key,
	mysql_conn_id = "mysql_conn_id",
	aws_conn_id = "aws_conn_id",
	verify = False,
	dag = dag
)

```

- mysql_conn_id
	- mysql에 접속하기 위해 connection id를 지정 
- aws_conn_id
	- S3에 접속하기 위해 connection id를 지정
- s3_bucket
	- s3 bucket 주소
	- top-level-directory
	- ex) "grepp-data---"
- s3_key
	- directory
	- ex) "schema-table"

```python

s3_to_redshift_nps = S3ToRedshiftOperator(
	task_id = 's3_to_redshift_nps',
	s3_bucket = s3_bucket,
	s3_key = s3_key,
	schema = schema,
	table = table,
	copy_options=['csv'],
	redshift_conn_id = "redshift_dev_db",
	dag = dag
)

```

- redshift_conn_id
	- redshift에 접속하기 위해 connection id를 지정
- copy_options
	- MySQLToS3Operator가 file을 S3에 올릴 때 csv 파일로 올림.
	- COPY command를 실행할 때, 파일 포멧을 알려줘야 함.
- schema
- table
	- redshift 안에 어느 table에 저장할 것인지.

#### ㄱ) 에러

- 처음 실행하면 잘 동작함.
- 2번째 실행부터 S3 키가 존재한다는 에러가 발생함
	- MySQLToS3Operator는 덮어쓰기 기능이 없다.
		- 지금 업로드 하려고 하는 S3 bucket 밑에 S3 path에 S3 key에 뭔가가 존재한다면 에러가 발생함.

### b. MySQL_to_Redshift (에러 개선)

- 3개의 Operator를 사용해서 구현
	- S3DeleteObjectOperator
	- MySQLToS3Operator
	- S3ToRedshiftOperator

---

```python

# s3_key가 존재하지 않으면 에러를 냄!

s3_folder_cleanup = S3DeleteObjectsOperator(
	task_id = 's3_folder_cleanup',
	bucket = s3_bucket,
	keys = s3_key,
	aws_conn_id = "aws_conn_id",
	dag = dag
)

```

```python

s3_to_redshift_nps = S3ToRedshiftOperator(
	task_id = 's3_to_redshift_nps',
	s3_bucket = s3_bucket,
	s3_key = s3_key,
	schema = schema,
	table = table,
	copy_options=['csv'],
	truncate_table = True,
	redshift_conn_id = "redshift_dev_db",
	dag = dag
)

```

- truncate_table
	- default가 False
	- table을 새로만드는 것이 아닌, record들을 계속 새로 적재한다.
		- full refresh와 같은 효과

---

Airflow에서 기본으로 제공되는 S3 관련된 기능들이 그렇게 좋지 않음.  
그래서 많은 경우 python에서 AWS를 조작하는데 사용되는 **boto3 library**를 사용하여 직접 구현함.

### c. MySQL_to_Redshift (Incremental Update)

> Incremental Update가 가능하고, Back fill을 하는데 사용할 수 있다.

- MySQL/PostgreSQL 테이블이라면 다음을 만족해야함
	> 해당 record가 언제 바꼈는지에 대한 timestamp가 있어야 한다.
	- created (timestamp)
		- Optional
	- modified (timestamp)
		- record가 수정될 가능성이 전혀 없는 경우에는 modified timestamp 필드가 필요 없음.
	- deleted (boolean)
		- 레코드를 삭제하지 않고 delete를 True로 설정
		- record를 physical하게 삭제하면, INCREMENTAL UPDATE를 할 수 없음.
		- 개인정보와 같은 보안 이슈가 있지 않는 한, table을 physical하게 delete하는 것은 좋지 않음. 
- Daily Update이고 테이블의 이름이 A이고 MySQL에서 읽어온다면
	- 먼저 Redshift의 A테이블의 내용을 temp_A로 복사
	- MySQL의 A테이블의 레코드 중 **modified의 날짜가 지난 일(execution_date)에 해당**하는 모든 레코드를 읽어다가 temp_A로 복사
		- `SELECT * FROM A WHERE DATE(modified) = DATE({execution_date})`
			- back fill을 하려면, airflow에서 넘겨주는 execution date을 보고 modified timestamp를 정한다.
	- temp_A의 레코드들을 primary key를 기준으로 파티션한 다음에 modified 값을 기준으로 DESC 정렬해서, 일련번호가 1인 것들만 다시 A로 복사

---

- MySQL에 있는 테이블 nps를 Redshift내의 스키마 밑의 nps 테이블로 복사
- Variable로 iam_role_for_copy_access_token 추가
	- `arn:aws:iam::0807***:role/redshift.read.s3`
		- 이 권한의 생성은 [Redshift에게 위 S3 bucket에 대한 액세스 권한 지정](https://docs.google.com/document/d/1FArSdUmDWHM9zbgEWtmYSJnxPXDX-LB7HT33AYJlWIA/edit#heading=h.9u82ph29nth9) 참고
	- 이는 COPY 명령을 실행할 수 있는 권한이 있음을 보여주기 위해 사용됨

```
COPY mildsalmon_su.nps
FROM 's3://grepp-data-****/mildsalmon-nps'
with credentials 'aws_iam_role=***'
csv;

# S3 위치에 있는 파일을 Redshift 위치에 복사해라.
COPY {Destination 위치}
FROM {S3 위치}
with credentials 'aws_iam_role=***'
csv;

```

- COPY 명령은 중복제거, full refresh 신경쓰지 않고, S3 위치에 있는 파일을 Redshift에 loading하는 것만 함.
- with credentials
	- 이 명령을 실행하는 사람이 S3에 대한 접근 권한이 있다는 것을 지정
- csv
	- S3에 있는 파일의 포멧 지정

---

- 3개의 Operator를 사용해서 구현
	- S3DeleteObjectsOperator
	- MySQLToS3Operator
		- execution_date에 해당하는 레코드만 읽어오게 바뀜
	- 자체 구현한 S3ToRedshiftOperator
		- plugins 폴더

---

```python

mysql_to_s3_nps = MySQLToS3Operator(
	task_id = 'mysql_to_s3_nps',
	query = "SELECT * FROM prod.nps WHERE DATE(created_at) = DATE('{{ execution_date }}')",
	s3_bucket = s3_bucket,
	s3_key = s3_key,
	mysql_conn_id = "mysql_conn_id",
	aws_conn_id = "aws_conn_id",
	verify = False,
	dag = dag
)

```

- DATE('{{ execution_date }}')
	- `{{ }}`는 flask의 Jinja Template 표기법
	- pythonOperator를 사용할때는 PythonOperator 함수의 Context 파라미터의 execution date를 읽어왔는데, Jinja Template을 사용해서 얻어올 수도 있음
	- `{{ }}` 안에 있는 변수를 Airflow Variable로 치환을 해줌.
		- airflow variable은 [Templates reference — Airflow Documentation (apache.org)](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html) 여기에 나와 있음

> mysql_to_s3_nps TASK가 실행되면, s3 bucket에 csv 파일을 떨구게 됨.

---

```python

s3_to_redshift_nps = S3ToRedshiftOperator(
	task_id = 's3_to_redshift_nps',
	s3_bucket = s3_bucket,
	s3_key = s3_key,
	schema = schema,
	table = table,
	copy_options=['csv'],
	redshift_conn_id = "redshift_dev_db",
	primary_key = "id",
	order_key = "created_at",
	dag = dag
)

```


> Upsert
> > 업데이트를 진행할 때, 만족하는 row가 있다면 update를 하고, 없다면 insert를 하는 것을 의미

- 자체 구현한 S3ToRedshiftOperator
	- primary key와 order key를 가지고 같은 partition 안에서 ordering을 함
	- 기존에 있는 S3ToRedshiftOperator는 Upsert 기능이 없음.
		- 중복이 생겼을 때, 중복을 제거해주는 기능이 없음


# 3. How to Backfill in Airflow

## A. Backfill을 커맨드 라인에서 실행하는 방법

> 보통 Backfill은 커맨드에서 실행함.

```
airflow dags backfill dag_id -s 2018-07-01 -e 2018-08-01
```

- This assumes the followings
	- catchup이 True로 설정되어 있음
	- execution_date을 사용해서 Incremental update가 구현되어 있음
		- full refresh한 DAG이면 backfill할 이유가 없음
- start_date 부터 시작하지만 end_date는 포함하지 않음

## B. How to Make Your DAG Backfill ready

- 먼저 모든 DAG가 backfill을 필요로 하지는 않음
	- Full Refresh를 한다면 backfill은 의미가 없음
- 여기서 backfill은 일별 혹은 시간별로 업데이트하는 경우를 의미함
	- 마지막 업데이트 시간 기준 backfill을 하는 경우라면 (Data Warehouse 테이블에 기록된 시간 기준) 이런 경우에도 execution_date를 이용한 backfill은 필요하지 않음
- 데이터의 크기가 굉장히 커지면 backfill 기능을 구현해 두는 것이 필수
	- airflow가 큰 도움이 됨
	- 하지만 데이터 소스의 도움 없이는 불가능
- 어떻게 backfill로 구현할 것인가?
	- 제일 중요한 것은 데이터 소스가 backfill 방식을 지원해야함
	- "execution_date"을 사용해서 업데이트할 데이터 결정
	- "catchup" 필드를 True로 설정
	- start_date/end_date을 backfill하려는 날짜로 설정
	- 다음으로 중요한 것은 DAG 구현이 execution_date을 고려해야 하는 것이고 idempotent 해야함

## C. 마지막으로 읽어온 레코드를 기반으로한 Incremental Update

- 가장 최근 시각의 레코드나 가장 큰 레코드 ID를 기반으로 구현하는 것이 한 방법
	- DW에서 해당 데이터의 마지막 timestamp나 ID를 읽어온다.
	- 데이터 소스에서 그 이후로 만들어지거나 변경이 있는 레코드들만 읽어온다.
	- 이를 DW의 테이블에 추가한다. (append)
	- 만일 테이블이 Append only가 아닌 경우 변경이 생긴 레코드들의 경우 중복이 존재하기에 이를 제거한다.
		- 가장 나중에 추가된 레코드만 남기고 나머지는 제거

# 4. Summary Table 구현

## A. Summary Table : 간단한 DAG 구현을 살펴보기

- 이 부분을 DBT로 구현하는 회사들도 많음 (Analytics Engineer)
	- https://www.getdbt.com/

---

- Summary Table을 만들때는 임시 테이블을 만들고 다양한 테스트를 해야 한다.
	- 레코드가 1개 이상 있는가? 등
	- 테스트가 성공하면, 그때 테이블을 교체한다.

```python

cur.execute("SELECT COUNT(1) FROM {schema}.temp_{table}""".format(schema=schema, table=table))
count = cur.fetchone()[0]
if count == 0:
	raise ValueError("{schema}.{table} didn't have any record".format(schema=schema, table=table))

```

params에 들어가는 내용을 airflow configuration에 지정해서 코드 변경없이도 할 수 있음.