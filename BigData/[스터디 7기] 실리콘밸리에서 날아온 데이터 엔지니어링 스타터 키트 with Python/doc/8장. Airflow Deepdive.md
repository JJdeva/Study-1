# 0. 4주차 숙제 리뷰

## A. Python ETL 개선

- Task를 Airflow에서는 Operator라고 부름

- Header를 제거하기 위해 `text.split("\n")[1:]`
- 데이터 멱등성을 위해 `DELETE FROM mildsalmon_su.name_gender;`
- 트랜잭션의 원자성을 위해 `BEGIN ~ END`로 트랜잭션을 만듬.
	- 트랜잭션이 실행되는 동안(`BEGIN`)에 변경된 사항은 메모리에만 적용하고 트랜잭션이 끝나면`END(=commit)`되면 디스크에 적용한다.
- 트랜잭션 진행 중에 문제가 생기면, rollback;을 입력하거나 connection으로 연 세션이 종료되면 자동으로 rollback된다.

```python

def extract(url):
	logging.info("Extract started")
	f = requests.get(url)
	logging.info("Extract done")
	
	return (f.text)

def transform(text):
	logging.info("transform started")
	# ignore the first line - header
	lines = text.split("\n")[1:]
	logging.info("transform done")

	return lines

def load(lines):
	logging.info("load started")
	cur = get_Redshift_connection()
	sql = "BEGIN;DELETE FROM mildsalmon_su.name_gender;"
	for l in lines:
		if l != '':
			(name, gender) = l.split(",")
			sql += f"INSERT INTO mildsalmon_su.name_gender VALUES ('{name}', '{gender}');"
	sql += "END;"
	cur.execute(sql)
	logging.info(sql)
	logging.info("load done")

def etl():
	link = "https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv"
	data = extract(link)
	lines = transform(data)
	load(lines)

```

## B. 트랜잭션이란?

- **Atomic하게 실행**되어야 하는 SQL들을 묶어서 **하나의 작업처럼 처리**하는 방법
	- BEGIN과 END 혹은 BEGIN과 COMMIT 사이에 해당 SQL들을 사용
	- ROLLBACK
- 실제로 DB에 update, insert, delete하는 부분만 트랜잭션으로 잡아야한다.

![](/bin/DE7_image/DE7_8_3.png)

## C. 트랜잭션 구현방법

> 레코드 변경을 바로 반영하는지 여부
> autocommit 이라는 파라미터로 조절 가능

- autocommit = True
	- 기본적으로 모든 SQL statement가 바로 커밋됨
	- 이를 바꾸고 싶다면 BEGIN; END; 혹은 BEGIN; COMMIT;을 사용 (혹은 ROLLBACK)
- autocommit = False
	- 기본적으로 모든 SQL statement가 커밋되지 않음
	- 커넥션 객체의 .commit()과 .rollback()함수로 커밋할지 말지 결정

### a. Error catch하는 방법

- Python의 경우 try/catch와 같이 사용하는 것이 일반적.
- error이 발생하면 connection 세션이 끊어지고 재사용되지 않는다. 

# 1. 간단한 Airflow job 실행하기

## A. my_first_dag

### a. DAG

- 2개의 task로 구성된 데이터 파이프라인(DAG)
	- print_hello
		- PythonOperator로 구성되어 있으며 먼저 실행
	- print_goodbye
		- PythonOperator로 구성되어 있으며 두번째로 실행

```python

dag = DAG(
 dag_id = 'my_first_dag',
 start_date = datetime(2021,8,26),
 catchup=False,
 tags=['example'],
 schedule_interval = '0 2 * * *'
)

```

- dag_id
	- 실제로 production에서 사용할 때는 dag 이름에 convention을 갖는게 좋음
	- dag python파일 이름과 dag 이름이 같은게 좋음
		- dag가 몇백개 되면, dag를 찾는게 일이 됨.

![](/bin/DE7_image/DE7_8_4.png)

### b. task

```python

print_hello = PythonOperator(
	task_id = 'print_hello',
	python_callable = print_hello,
	dag = dag
)

print_goodbye = PythonOperator(
	task_id = 'print_goodbye',
	python_callable = print_goodbye,
	dag = dag
)

```

- task_id
	- task_id를 지정하면 DAG 안에서 특정 task의 식별자가 된다.
- python_callable
	- 실행시킬 함수

![](/bin/DE7_image/DE7_8_5.png)

#### ㄱ) task 실행 순서

```python

print_hello >> print_goodbye

```

print_hello 다음에 print_goodbye가 실행됨.

- task 실행 순서를 적어주지 않으면, task가 각각 개별적으로 실행됨.
	- 순서가 없음

## B. How to Trigger a DAG - 터미널에서 실행

- 먼저 SSH로 Airflow 서버에 로그인하고 airflow 사용자로 변경
	- `airflow dags list`
	- `airflow tasks list [DAG이름]`
	- `airflow tasks test [DAG이름] [task이름] [execution date]`
		- 날짜는 `YYYY-MM-DD`
			- start_date보다 과거인 경우는 실행이 되지만 오늘 날짜보다 미래인 경우 실행 안됨
			- 이게 바로 execution_date의 값이 됨
	- `airflow tasks list`
	- `airflow tasks test [DAG이름] [task이름] [execution date]`
	- `airflow dags test [DAG이름] [execution date]`
	- `airflow dags backfill [DAG이름] -s [start date] -e [end date]`

# 2. Airflow Operators, Variables and Connections

## A. DAG Configuration Example

### a. Common Configuration

```python

from datetime import datetiome, timedelta
from airflow import DAG

default_args = {
	'owner': 'mildsalmon_su',
	'email': ['mildsalmon@gmail.com'],
	'retries': 1,
	'retry_delay': timedelta(minutes=3),
}

```

> This is applied to the task level

- retries
	- 실패했을 경우, retry 횟수
- retry_delay
	- retry 사이에 기다리는 시간
- on_failure_callback
	- 실패했을 경우 동작

---

- start_date and end_date specify when this dag starts and stops:
	- can be used to do one-off backfilling
	- Understanding implication of this is important along with catchup

---

- start_date와 end_date는 이 dag가 시작되고 끝나기를 지정한다.
	- 1회성 backfill에 사용될 수 있다.
	- `catchup`과 함께 이것의 의미를 이해하는 것이 중요하다.

## B. DAG Object Creation Example

```python

dag_second_assignment = DAG(
	# DAG name
	dag_id = 'second_assignment_v4',
	start_date = datetime(2021,11,27, hour=0, minute=0), # 날짜가 미래인 경우 실행이 안됨
	# schedule (same as cronjob)
	schedule_interval = '0 2 * * *',
	# common settings
	default_args = default_args
)

```

- Schedule interval can be defined as cron expression or presets as follows
	- None, @once, @hourly, @daily, @weekly, @monthly, @yearly
- Note that Airflow will run the first job at start_date + interval
	- A daily dag with the start_date '2020-07-26 01:00' starts at 2020-07-27 01:00.
	- A hourly dag with the start_date 'for '2020-07-26 01:00' start s at 2020-07-26 02:00
	- This causes a lot of confusion in terms of figuring out when a DAG will be triggered for the first time (combined with UTC if you use UTC)

## C. Important DAG parameters (not task parameters)

- max_active_runs:
	- number of DAGs instance
	- 한번에 DAG에 몇개의 instance가 동시에 돌 수 있는지?
	- backfill 할 때 중요
- concurrency:
	- number of tasks that can run in parallel
	- dag 안에서 동시에 돌 수 있는 task의 수
- catchup:
	- whether to backfill past runs

### a. DAG parameters vs Task parameters의 차이점 이해가 중요

- 위의 파라미터들은 모두 DAG파라미터로 DAG 객체를 만들 때 지정해주어야함
- default_args로 지정해주면 에러는 안나지만, 적용이 안됨
	- default_args로 지정되는 파라미터들은 task level로 적용되는 파라미터들

## D. Operators - PythonOperator

```python

from airflow.exceptions import AirflowException

def python_func(**cxt):
	table = cxt["params"]["table"]
	schema = cxt["params"]["schema"]
	ex_date = cxt["execution_date"]

load_nps = PythonOperator(
	dag=dag,
	task_id = 'task_id',
	python_callable=python_func,
	params={
		'table': 'delighted_nps',
		'schema': 'raw_data'
	},
	provide_context=True
)

```

- provide_context
	- Setting provide_context to True enables passing **params** dictionary (in red) to the corresponding Python function (Python_func here)

---

- python_callable 함수에 파라미터로 넘길때
	- **provide_context의 값을 True**로 지정
	- **params에 인자들을 dict형태**로 지정
- python 함수에서는 cxt에서 params dict 안에 인자값을 받아올 수 있음
	- cxt를 통해 airflow에서 유지하는 다양한 system variable들을 읽어올 수 있음
		- ex) execution_date

# 3. Name Gender DAG 개선하기

```python

def get_Redshift_connection():
    host = 
    user = 
    password =  
    port = 5439
    dbname =
    conn = psycopg2.connect(f"dbname={dbname} user={user} host={host} password={password} port={port}")
    conn.set_session(autocommit=True)
    return conn.cursor()


def extract(url):
    logging.info("Extract started")
    f = requests.get(url)
    logging.info("Extract done")
    return (f.text)


def transform(text):
    logging.info("transform started")
    # ignore the first line - header
    lines = text.split("\n")[1:]
    logging.info("transform done")
    return lines


def load(lines):
    logging.info("load started")
    cur = get_Redshift_connection()
    sql = "BEGIN;DELETE FROM keeyong.name_gender;"
    for l in lines:
        if l != '':
            (name, gender) = l.split(",")
            sql += f"INSERT INTO keeyong.name_gender VALUES ('{name}', '{gender}');"
    sql += "END;"
    cur.execute(sql)
    logging.info(sql)
    logging.info("load done")


def etl():
    link = ""
    data = extract(link)
    lines = transform(data)
    load(lines)


dag_second_assignment = DAG(
	dag_id = 'second_assignment',
  catchup = False,
	start_date = datetime(2021,11,27), # 날짜가 미래인 경우 실행이 안됨
	schedule_interval = '0 2 * * *')  # 적당히 조절

task = PythonOperator(
	task_id = 'perform_etl',
	python_callable = etl,
	dag = dag_second_assignment)

```

## A. NameGenderCSVtoRedshift.py 개선하기 \#1

- params를 통해 변수 넘기기
- execution_date 얻어내기
- `delete from` vs `truncate`
	- `DELETE FROM raw_data.name_gender; --WHERE 사용 가능`
	- `TRUNCATE raw_data.name_gender;`


```python

def etl(**context):
    link = context["params"]["url"]
    # task 자체에 대한 정보 (일부는 DAG의 정보가 되기도 함)를 읽고 싶다면 context['task_instance'] 혹은 context['ti']를 통해 가능
    # https://airflow.readthedocs.io/en/latest/_api/airflow/models/taskinstance/index.html#airflow.models.TaskInstance
    task_instance = context['task_instance']
    execution_date = context['execution_date']

    logging.info(execution_date)

    data = extract(link)
    lines = transform(data)
    load(lines)

task = PythonOperator(
	task_id = 'perform_etl',
	python_callable = etl,
        params = {
            'url': "https://s3-geospatial.s3-us-west-2.amazonaws.com/name_gender.csv"
        },
        provide_context=True,
	dag = dag_second_assignment)

```

## B. NameGenderCSVtoRedshift.py 개선하기 \#2

- Xcom 객체를 사용해서 세 개의 task로 나누기
- Redshift의 스키마와 테이블 이름을 params로 넘기기

### a. Variable

```python

from airflow.models import Variable

extract = PythonOperator(
	task_id = 'extract',
	python_callable = extract,
	params = {
		'url': Variable.get("csv_url")
	},
	provide_context=True,
	dag = dag_second_assignment
)

```

- Variable.get(\[key\])
	- task에서 url을 하드코딩하지 않고 airflow의 Variable에서 읽어오도록 함.
- Variable.set(\[key\])
	- Variable의 값을 갱신할 수 있음.

![](/bin/DE7_image/DE7_8_6.png)

- Variable은 Web UI에서 설정가능한데, Encryption이 가능함.

---

- Variables
	- Used to store API keys or some configuration info
	- Use "access" or "secret" in the name if you want its value to be encrypted

### b. Connections

- 외부 데이터베이스 등의 로그인이 필요한 시스템들을 연결할 때의 정보들을 적어줌.

```python

from airflow.hooks.postgres_hook import PostgresHook

def get_Redshift_connection():
	hook = PostgresHook(postgres_conn_id='redshift_dev_db')
	return hook.get_conn().cursor()

```

---

- Connections
	- This is used to store some connection related info such as host name, port number and access credential
	- Postgres connection or Redshift connection info can be stored here

### c. Xcom 객체

Xcom Object를 사용하여 앞에 실행된 task에서 어떤 값들을 읽어올 수 있음

```python

def transform(**context):
    text = context["task_instance"].xcom_pull(key="return_value", task_ids="extract")
    lines = text.split("\n")[1:]
    return lines

```

- xcom_pull(key='', task_ids='')
	- task_ids에 해당하는 task에서 key(return한 값)를 가져와라

#### ㄱ) task를 나누면 - 장점 단점

##### 1) 단점

- 복잡도가 늘어남.
	- 각 task들이 스케줄됨.
		- airflow가 더 바빠지고 실행 시간이 좀 더 늘어날 수 있음.
	- 각 task들 간에 데이터를 넘겨주는 것이 명확하게 보이지 않음.
	- 넘겨주는 데이터가 엄청 크다면, 어딘가에 저장하고 데이터의 path를 리턴하고 읽어오는 방식으로 구현한다.

##### 2) 단점

- 중간에 task가 실패한 부분부터 다시 실행할 수 있음.

# 4. Primary Key Uniqueness 보장하기

> 데이터 웨어하우스들은 Primary Key를 보장하지 않음

```sql

CREATE TABLE mildsalmon_su.weather_forecast (
 date date primary key,
 temp float,
 min_temp float,
 max_temp float,
 created_date timestamp default GETDATE()
);

```

 - 임시 테이블(스테이징 테이블)을 만들고 거기로 현재 모든 레코드를 복사
 - 임시 테이블에 새로 데이터소스에서 읽어들인 레코드들을 복사
	 - 이때는 중복 데이터 존재할 수 있음
 - 중복을 걸러주는 SQL 작성
	 - ROW_NUMBER를 이용해서 primary key로 partition을 잡고 다른 필드(보통 타임스탬프)로 ordering을 수행해 primary key별로 하나의 레코드를 찾아냄
 - 위의 SQL을 바탕으로 최종 원본 테이블로 복사 (SWAP)
	 1. 원본 테이블을 DROP하고, 임시 temp 테이블을 원본 테이블로 바꿔주기 (ALTER TABLE)
	 2. 원본 테이블 내용을 전부 DELETE하고, 임시 temp 테이블을 INSERT 해주기

---

1. `CREATE TABLE [schema].[temp table] AS SELECT ~ FROM [schema].[table];`
	- 원래 테이블의 내용을 temp table로 복사
2. DAG는 임시 테이블(스테이징 테이블)에 레코드를 추가
	- 이때 중복데이터가 들어갈 수 있음
3. `DELETE FROM [schema].[table];`
4. `INSERT INTO [schema].[table] SELECT ~ FROM (SELECT ~, ROW_NUMBER() OVER (PARTITION BY date ORDER BY create_date DESC) seq FROM [schema].[temp table]) WHERE seq = 1;`

> 매번 새로 덮어쓰는 형식의 업데이트를 가정

- 3, 4번은 transaction으로 처리하는게 좋음.
	- 3, 4번 실패시 전부 실패되는 것이 원자성을 보장해줌
- 데이터 엔지니어링 관점에서는, 만약 1, 2번에서 실패하면, error 발생시키고 다시 시작하면 됨.
	- 1, 2번이 실패한다고 table의 문제가 생기지는 않음.
	- 완벽하게 복구를 못할거면 그냥 fail하는게 나음.
	- 에러가 발생하면, 코드 안에서 recovery하지 말고 에러를 보여주는게 나음.