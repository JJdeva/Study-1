# Table of Contents

- [0. 6주차 숙제 리뷰](#0-6주차-숙제-리뷰)
- [1. Airflow Configuration for Production Usage](#1-airflow-configuration-for-production-usage)
- [2. Slack 연동하기](#2-slack-연동하기)
- [3. API & Airflow 모니터링](#3-api--airflow-모니터링)
- [4. Google Spreadsheet 연동하기](#4-google-spreadsheet-연동하기)
- [5. AWS/Airflow 관련 보안](#5-awsairflow-관련-보안)
- [7. Next Steps](#7-next-steps)

---

# 0. 6주차 숙제 리뷰

## A. 어떤 Airflow 버전을 사용해야 좋을까?

> 큰 회사들이 사용하는 버전을 사용하는 것이 좋음.

[https://cloud.google.com/composer/docs/concepts/versioning/composer-versions](https://cloud.google.com/composer/docs/concepts/versioning/composer-versions) 참고

## B. NPS SQL을 주기적으로 요약 테이블로 만들기

### a. DAG 안에 params로 query 입력

이 방식은 간단하지만, 데이터 분석가가 직접 입력하기에는 부담이 있을 수 있다.

```python

execsql = PythonOperator(  
    task_id='execsql',  
 python_callable=execSQL,  
 params={  
        'schema': 'mildsalmon_su',  
 'table': 'nps_summary',  
 'sql': """  
 SELECT LEFT(created_at, 10) , ROUND(SUM(CASE WHEN score >= 9 THEN 1 WHEN score <= 6 THEN -1 END)::float / COUNT(1) * 100, 2) FROM mildsalmon_su.nps GROUP BY 1 ORDER BY 1 ; """ },  
 provide_context=True,  
 dag=dag  
)

```

### b. config 파일로 만들기

> 코드라기 보다는 환경 설정 파일을 만들어서 설정한다.

파이썬 딕셔너리를 만들어서 params를 입력하고 DAG에 가져와서 사용한다.

```python

{
	'table': 'nps_summary',
	'schema': 'keeyong',
	'main_sql': """SSELECT LEFT(created_at, 10) AS date,
				ROUND(SUM(CASE
				WHEN score >= 9 THEN 1
				WHEN score <= 6 THEN -1 END)::float*100/COUNT(1), 2)
				FROM keeyong.nps
				GROUP BY 1
				ORDER BY 1;""",
	'input_check':
	[
		{
			'sql': 'SELECT COUNT(1) FROM keeyong.nps',
			'count': 150000
		},
	],
	'output_check':
	[
		{
			'sql': 'SELECT COUNT(1) FROM {schema}.temp_{table}',
			'count': 12
		}
	],
}

```

summary table을 만들기 전에 input_check를 통해 원하는 형태의 조건을 만족하는지를 체크한다.

- input_check
	- CTAS하기 전에 실행된다.
	- list 형태로 만들어져서, 다수의 test를 붙일 수 있다.
- output_check
	- 최종적으로 만들어진 table에 다양한 조건들을 확인할 수 있다.
	- primary key uniqueness하다는 것을 검증한다.
- input_check나 output_check가 실패하면 summary table을 만드는 작업을 Fail하고 이전 상태를 유지한다.

### c. DBT 사용하기 (Analytics Engineering (ELT))

위 과정을 쉽고 scalable하게 만든 것이 DBT

# 1. Airflow Configuration for Production Usage

## A. Things to Change

### a. airflow.cfg is in /var/lib/airflow/airflow.cfg

- Any changes here will be reflected when you restart the **webserver** and **scheduler**
- [core] 섹션의 dags_folder가 DAG들이 있는 디렉토리가 되어야함
	- /var/lib/airflow/dags
- dag_dir_list_interval
	- dags_folder를 Airflow가 얼마나 자주 스캔하는지 명시 (초 단위)

#### ㄱ) airflow.cfg 백업

airflow metadata database를 암호화하는데 사용하는 key가 airflow.cfg 파일에 있음.

그래서 airflow.cfg 파일은 주기적으로 백업을 해둬야함.

### b. Airflow Database upgrade

- sqlite -> Postgres OR MySQL
- sql_alchemy_conn in Core section of airflow.cfg

### c. LocalExecutor 사용

- Executor in Core section of airflow.cfg
	- Single Server
		- from SequentialExcecutor to LocalExecutor
	- Cluster
		- from SequentialExecutor to CeleryExecutor or KubernetesExecutor

### d. Enable Authentication

- A horror story to share
	- Airflow 1.0은 초기 로그인 화면이 default가 아니였음.
- In Airflow 2.0, authentication is ON by default

### e. Large disk volume for logs and local data

- Logs -> /dev/airflow/logs in (Core section of airflow.cfg)
	- base_log_folder
	- child_process_log_directory
- Local data -> /dev/airflow/data

---

- local 등의 환경에서 직접 manage할 때부터는 disk volume이 충분히 있는지를 확인하는 것이 중요함.
- log 파일들은 local에서 삭제하거나, S3에 별도로 업로드해서 관리하는 것이 좋음.

### f. Periodic Log data cleanup

- The above folders need to be cleaned up periodically
- You can write a shell Operator based DAG for this purpose

### g. From Scale Up to Scale Out

- Go for Cloud Airflow option

---

- worker가 여러개여도 scheduler는 한개임.
	- master scheduler가 죽으면 secondary scheduler가 실행되도록 설정하는 이중화가 필요
	- Airflow 2.0 부터는 이런 것들을 보장하는 기능이 있음.

### h. Backup Airflow database

- Airflow.cfg needs to be backed up as well (encryption key)
	- **fernet_key** in core section
- Backup variables and connections (command lines or APIs)
	- airflow variables export variables.json
	- airflow connections export connections.json

### i. Add health-check monitoring

- [https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/check-health.html](https://airflow.apache.org/docs/apache-airflow/stable/logging-monitoring/check-health.html)
	- API를 먼저 활성화하고 Health Check Endpoint API를 모니터링 툴과 연동

---

- Scheduler가 살아있는지를 확인하는 monitor
	- Scheduler가 정상적으로 동작하는지를 확인하기 위해 Running 중인 Dag 수가 몇 개인지 체크
- Webserver가 살아있는지를 확인하는 monitor

# 2. Slack 연동하기

## A. 데이터 파이프라인 문제를 슬랙에 표시

- [https://api.slack.com/messaging/webhooks](https://api.slack.com/messaging/webhooks)
	- 위를 따라해서 Incoming Webhooks App 생성

## B. Webhook으로 메세지 보내기

```
curl -X POST -H 'Content-type: application/json' --data '{"text":"Hello, World!"}' https://hooks.slack.com/services/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
```

## C. 데이터 파이프라인 실패/경고를 슬랙으로 보내는 방법

- `T010X0Q2B5W/B037F7V21HD/Xu2cd5IUXbj19D1oaCSWLOqU`을 **slac_url** Variable로 저장
- slack에 에러 메세지를 보내는 별도 모듈로 개발
	- slack.py
- 이것을 DAG 인스턴스를 만들 때 에러 콜백(on_failure_callback, on_success_callback)으로 지정

```python

dag_second_assignment = DAG(
	dag_id = 'second_assignment_v4',
	start_date = datetime(2021,11,27), # 날짜가 미래인 경우 실행이 안됨
	schedule_interval = '0 2 * * *', # 적당히 조절
	max_active_runs = 1,
	catchup = False,
	default_args = {
		'retries': 1,
		'retry_delay': timedelta(minutes=3),
		'on_failure_callback': slack.on_failure_callback,
	}
)

```

# 3. API & Airflow 모니터링

## A. [Airflow API 활성화](https://airflow.apache.org/docs/apache-airflow/stable/security/api.html)

- airflow.cfg의 api 섹션에서 auth_backend의 값을 변경

```

[api]
auth_backend = airflow.api.auth.backend.basic_auth

```

- airflow 스케줄러 재실행

```
   
sudo systemctl restart airflow-webserver
sudo systemctl restart airflow-scheduler # 아마도 불필요?

```

- basic_auth의 ID/Password 설정
	- Airflow 서버로 로그인해서 airflow 사용자로 변경(sudo su airflow)
	```
	
	airflow config get-value api auth_backend
	
	```

- Airflow Web UI에서 새로운 사용자 추가
	- Security -> List Users -> + -> 이후 화면에서 새 사용자 정보 추가 (monitor:MonitorUser1)

## B. Health API 호출

- /health API 호출 (앞서 생성한 사용자 정보 사용)

```
curl -X GET --user "Monitor:MonitorUser1" http://[AirflowServer]:8080/health
```

- 정상 응답

```

{
	"metadatabase": {
		"status": "healthy"
	},
		"scheduler": {
			"status": "healthy",
			"latest_scheduler_heartbeat": "2022-03-12T06:02:38.067178+00:00"
	}
}

```

- 응답이 온다는 것
	- Web Server가 살아있다는 의미
- metadatabase의 status
	- airflow가 사용하는 metadatabase의 이상 유무
- 스케줄러의 status
	- 스케줄러가 healthy여도 heartbeat 값이 최신이 아니면, 작동 안하는 것일 수도 있음.

## C. API 사용예

- [특정 DAG를 API로 Trigger하기](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/post_dag_run)
- [모든 DAG 리스트하기](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#operation/get_dags)
- [모든 Variable 리스트하기](https://airflow.apache.org/docs/apache-airflow/stable/stable-rest-api-ref.html#tag/Variable)

# 4. Google Spreadsheet 연동하기

## A. 구글 서비스 어카운트 생성

- 구글 클라우드 로그인
	- [https://console.cloud.google.com/](https://console.cloud.google.com/)
- 구글 스프레드시트 API 활성화 필요
	- [https://console.cloud.google.com/apis/library/sheets.googleapis.com](https://console.cloud.google.com/apis/library/sheets.googleapis.com)
- 다음으로 구글 서비스 어카운트 생성 (JSON)
	- 아래 문서 참고
		- [https://robocorp.com/docs/development-guide/google-sheets/interacting-with-google-sheets](https://robocorp.com/docs/development-guide/google-sheets/interacting-with-google-sheets)
		- [https://denisluiz.medium.com/python-with-google-sheets-service-account-step-by-step-8f74c26ed28e](https://denisluiz.medium.com/python-with-google-sheets-service-account-step-by-step-8f74c26ed28e)
	- 이 JSON파일의 내용을 airflow에 google_sheet_access_token이란 이름의 Variable로 등록
	- 그리고 여기 JSON 파일을 보면 이메일 주소가 하나 존재. 이를 읽고 싶은 구글스프레드시트 파일에 공유
		- 이 이메일은 **iam.gserviceaccount.com**로 끝남

## B. 구글 시트를 테이블로 복사하는 예제

- 실제 스프레드시트와 연동하는 방법은 아래 코드 두 개를 참고
	- `Gsheet_to_Redshift.py`
	- `plugins/gsheet.py`

# 5. AWS/Airflow 관련 보안

[Redshift cluster 설정 - Google Docs](https://docs.google.com/document/d/1FArSdUmDWHM9zbgEWtmYSJnxPXDX-LB7HT33AYJlWIA/edit)

## A. IAM Role with S3 access to Redshift

- Redshift에 S3의 read only access만 줌.
	- S3에서 COPY해서 읽어오기만 하기 때문.
- UNLOAD
	- COPY와 반대되는 명령어
	- 특정 테이블의 내용을 S3에 쓰기 가능.
	- 특정 SELECT 내용을 S3에 쓰기 가능.

![](/bin/DE7_image/DE7_10_1.png)

![](/bin/DE7_image/DE7_10_2.png)

![](/bin/DE7_image/DE7_10_3.png)

## B. EC2 based IAM role

- IAM Role creation and attach it to EC2

![](/bin/DE7_image/DE7_10_4.png)

---

- Airflow가 동작하는 EC2에 S3를 access할 수 있는 IAM Role을 만들어서 명시적으로 지정해야함.
- EC2 밖에서 (local에서 Docker로) 접속하는 경우는 다른 방식으로 문제를 해결해야함.

# 7. Next Steps

## A. Q&A

- Redshift 설치는 google doc 참고
	- trial은 2달 무료
- Airflow metadata DB (Postgres라는 전제하)를 S3로 백업하는 DAG와 Log 폴더를 Cleanup하는 DAG도 Github에 추가됨
- airflow JINJA template으로 어떤 것들이 가능한지 정리된 링크
	- [Templates reference — Airflow Documentation (apache.org)](https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html)
- 코딩을 airflow로 하려면 ssh로 airflow로 바로 접속할 수 있게 해주는 방법
	- [Airflow 어카운트로 바로 로그인 - Google Slides](https://docs.google.com/presentation/d/1lhHsUHzVTekeaulj7hB7AamBceKAU-NRjBRx898JXgM/edit#slide=id.p)
- vscode로 remote로 접속하는 방법
	- [VSCode에서 SSH 원격 접속하는 방법 (tistory.com)](https://mmozzi.tistory.com/79)

## B. Data Engineering Side

> Python, SQL, Data Warehouse, ETL(Airflow), Spark (대용량 분산처리)

- 이번 강좌에서 배운 것은 DW를 기반으로 데이터 인프라를 만드는 것
- Data Analyst와 어떻게 협업해야 하는지에 집중 (Summary Table & Dashboard)
- 다음 스텝은 Data Scientist와 어떻게 협업을 해야하는지에 대해 배워보는 것
	- Feature 계산
		- 이는 큰 스케일에서는 Feature 계산을 위한 Spark와 같은 대용량 계산 프레임워크에 대한 지식이 필요
		- 작은 스케일에서는 Python이나 Pandas와 같은 라이브러리 사용이 필요
	- Model Serving
		- Data Scientist가 만든 모델을 어떻게 프로덕션으로 lunch할 수 있는지?
	- A/B Test 분석 파이프라인 (데이터 분석가와 협업)
- MLOps 와 ML Engineer
	- 코딩을 잘하는 Data Scientist