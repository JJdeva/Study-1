데이터 분석이 가능하려면 infra가 있어야 한다.

infra의 시작은 Data Warehouse이다.

Data Warehouse에 중요한 데이터들이 적재되면, 다양한 형태로 분석할 수 있다. 그러다보면 부족한 데이터를 어딘가에서 찾아서 적재할 수도 있고 새로 수집할 수도 있다.

데이터 엔지니어링의 시작은 Data Warehouse와 ETL이다. 여기에서 한단계 더 나아가면 Spark가 나온다.

# 1. What is Data Engineering?

## A. Different Roles

### a. Writing and Managing Data Pipelines

- **Data Warehouse를 관리**하는 것.
	- Data Warehouse에 data를 적재하는 코드를 작성해야 한다. (ETL = data pipeline = data job = DAG(airflow))
		- 데이터 소스에서 데이터를 추출하고 원하는 포멧으로 변환하고 data warehouse에 적재한다.
	- ETL이 몇 개 안되면, cronjob으로 실행한다.
	- ETL 수가 늘어나고, 실행 의존도가 생기면 Airflow로 처리한다.

### b. Types of Data Pipelines

- Batch Processing
- Realtime Processing
	- 스트리밍 데이터
- A/B Test Analysis
- Summary Data Generation
	- 요약 데이터 생성

### c. Event Collection

- User's behavioral data
	- 사용자 행동 데이터

### d. 세분화

- ML Engineer
	- 데이터 사이언티스트에 가까운 일을 함
	- 데이터 적재를 하고 Spark과 같은 대용량 프로세싱 툴을 통해 feature를 만들어내고, feature를 기반으로 machine learning 모델을 만듬
- MLOps
	- 모델을 만든 이후의 배포 과정 + 모니터링 과정을 담당한다.
	- 데이터 엔지니어링 지식 + 데브옵스 지식 + 데이터 사이언티스트 지식
- Privacy Engineer
	- 개인정보 

---

- Data Warehouse는?
	- Scalable한 관계형 데이터베이스
	- Redshift, BigQuery, Snowflake
	- Hadoop위에서 Spark 올리거나 Hive 올려서 DW로 사용할수도 있음
		- 이렇게 하면 manage해야할 것이 많아진다.
		- 그래서 cloud 기반의 DW로 가는 것이 전체적인 비용이나 효율성 측면에서 더 좋다.

## B. Skillset

### a. 반드시 알아야 하는 것

- **SQL**
	- Hive, Presto, SparkSQL
- Programming Language
	- **Python**, Scala, Java

### b. 빠른 성장을 위해 필요한 것

- Large Scale Computing Platform
	- **Spark**, YARN

### c. 알면 좋은 것

- 지식
	- Machine Learning, A/B test, 통계
- Cloud Computing
	- AWS
		- **Redshift**, EMR, S3, SagaMaker
	- GCP
		- **BigQuery**, ML Engine
	- Microsoft
		- AzureML
	- Snowflake
- ETL/ELT Scheduler
	- **Airflow**
- DBT
	- data transform

# 2. Typical Evolution of Data Team at a growing start-up

## A. In the Beginning

데이터가 없으니, 저렴한 SaaS 서비스를 이용한다.

## B. In the Growing Phase

- 데이터 분석가를 고용

### a. 하지만

- 필요한 데이터가 어디있는지 모르거나
- 깨끗하지 않고 누락되었거나
- 최적화되지 않은 저장소에 저장되었거나
	- production database에 저장된 데이터를 사용하라 하거나
		- 데이터 분석가가 데이터베이스에 큰 데이터를 조인하는 쿼리를 요청해서 웹 사이트를 몇 번 죽일 것이다.

---

- Production database
	- 웹 서비스를 만들면, 웹 서비스에 필요한 정보들이 저장된 데이터베이스
		- 사용자 정보, 상품 정보, 구매 정보 등 서비스가 운영되는데 필요한 정보들
	- MySQL, PostgreSQL 등
	- 처리하는 데이터의 양에는 제약이 있지만, 처리 속도는 빠르다.
		- 처리 속도에 초점이 맞춰져 있다. (OLTP)

## C. Step 1. Build Data Infra

> Upload Data To Data Warehouse

### a. Build Data Infra !

데이터 조직의 이상적인 발전 단계의 첫번째는 Data Infra를 만드는 것이다. 

- You have to find a **scalable** and **separate storage** for data analysis
	- This is called **Data Warehouse**
	- This will be the **central storage for your important data**

시작은 Data Warehouse이다.

즉, Production Database와는 별개의 Database이고 이 Database는 데이터 팀만 사용한다.
쿼리가 오래 돌아도 외부에는 영향이 없음.

- Migrate some data from MySQL
	- Key/Value data to NoSQL solution
	- Log type of data
	- Production DB should only have key data which is needed for its service

Data warehouse가 만들어지면, Production Database를 Data Warehouse로 옮겨온다.

### b. What is Data Warehouse?

- The goal is to put every data into a single storage^[목표는 모든 데이터를 단일 스토리지에 저장하는 것]
	- The very first step toward "true" data org^["진정한" 데이터 조직으로의 첫 번째 단계]
	- This storage should be separated from runtime storage (MySQL for example)^[이 저장소는 런타임 저장소(MySQL)와 분리되어야 한다.]
	- This storage should be scalable
	- Being consistent is more important than being correct in the begining^[처음에 정확했던 것보다 일관성이 있는 것이 중요하다.]

	---

- 회사에 필요한 모든 데이터를 저장해놓는 Storage
	- Production Database와 별개의 Database
	- Scalable하면 좋음.

### c. Now You Add More Data

- Different Ways of Collecting Data^[다양한 데이터 수집 방법]
	- This is called ETL (Extract, Transform and Load) or ELT
		- Also known as data pipeline^[데이터 파이프라인이라고도 함]
	- Different Aspects to Consider^[고려해야 할 다양한 측면]
		- Size
		- Frequency
		- Source Data Access
- You will have multiple data collection workflows
	- Use cronjob (or some scheduler) to manage at the beginning^[처음에는 cronjob같은 스케줄러를 사용하여 관리]
	- Airflow(Airbnb), Oozie(Yahoo), Azkaban(LinkedIn) can be an option
	
	---

- Data Warehouse가 만들어지면, 다양한 데이터를 Data Warehouse에 복사해야한다.
	- 복사해주는 코드가 ETL (Data Pipeline)
	- ETL이 많아지면, ETL 간에 실행 순서나 의존성 및 실패한 작업을 쉽게 실행하기 위해서 Airflow를 사용한다.

![](/bin/DE7_image/DE7_2_1.png)

Data Sources를 Airflow를 통해 Redshift에 복사한다. looker를 통해 대시보드를 만든다.

Redshift 내부에서 ETL을 돌려서 한 level 위에 abstract된 정보를 만들어서 looker 등에 사용한다.

이것이 summary tables.

summary table을 만드는 기능을 data infra(Redshift)내에 제공한다.

## D. Step2. Leverage Data

> Feedback Loops

### a. Build Summary Tables (periodically^[주기적])

- Having raw data tables is good but it can be too detailed and too much information^[원시 데이터 테이블이 있는 것은 좋지만, 너무 상세하고 정보가 많을 수 있다.]
- Build/Update these tables in your Data Warehouse periodically^[데이터 웨어하우스에서 정기적으로 이러한 테이블 구축/업데이트]
	- Append only vs. Build from scratch^[추가만 vs 처음부터 빌드]
- SQL Expertise is becoming really important here^[SQL 전문지식이 매우 중요해지고 있다.]
	- Redshift, Hive, BigQuery, SparkSQL
	- You can also take more programming approach (Spark)
- Timestamp : should be in HQ local timezone

	---

- summary table을 데이터 분석가가 사용하기 쉽게 만들어줘야한다.
- summary table 형태로 만들어놓고 대쉬보드에서 가져다 써야한다.
	- 이것으로 데이터 분석을 해야한다.

### b. Track the performance of key metrics

- This should be from summary tables above
- You need dashboard tool (build one or use 3rd party solution)
	- Looker, Tableau, Superset and so on
- Tableau is a lot more complicated but powerful
- Looker is the most popular nowadays

### c. Provide this data to Data Science team

- Draw insight and create feedback loop
- Build ML models for recommendation, search ranking and so on
	- Learn from the past and predict the future

	---

- 데이터 사이언티스트들이 데이터 인프라에 있는 데이터를 가지고 operation을 최적화하거나 우리 서비스를 사용하는 사람들의 경험을 개선할 수 있는 형태로 데이터를 사용한다.
- 데이터 팀 입장에서는 가장 눈에 띄는 성과를 보여줄 수 있음
	- 이것을 위해서 **데이터 인프라**가 있어야 한다.
	- 아무런 metrics(지표)없이 모델을 만들면, 얼마나 도움이 되는지 알 수 없다.
		- 그래서 **측정**하고 **시각화하는 툴**이 먼저 있어야 한다.
		- 대쉬보드가 있으면, 데이터 사이언티스트 팀이 만든 모델로 대쉬보드의 어떤 metric가 개선되는 것이 보인다.

> 인프라 구성 (데이터 분석가들이 지표를 만들기 쉬운 환경을 제공) -> 대시보드를 통해 우리가 한 일의 impact가 대시보드에 나와야 한다. -> 데이터 사이언티스트가 들어가서 모델을 만들경우 impact가 더 잘 보인다.

### d. Build Infrastructure for Data Science

- Building ML model and experimenting it will require scalable infrastructure^[ML 모델을 구축하고 실험하려면 확장 가능한 인프라가 필요하다.]
- Goal is to do iterative and repeatable experiments in short cycles^[목표는 반복적이고 반복가능한 실험을 짧은 주기로 수행하는 것이다.]
	- You need scalable Serving Layer (Cassandra, ElasticSearch ...)

# 3. Data Team at Udemy

## A. Data Warehouse/Analytics

- AWS Redshift was used as the data warehouse
	- what is AWS Redshift?
		- scalable PostgreSQL Engine up to 1.6PB of data
- Hive/Spark were added later
- Tableau was used as the BI tool

	---

- 처음에는 Redshift를 사용함
- Redshift만으로는 분석할 수 있는 데이터 타입이 제한되다보니 Spark를 도입함
- BI 도구로는 Tableau를 사용함.

## B. Types of Data Stored in Redshift

- Key tables from MySQL (Production Database)
- Data from Web access log
- Email Marketing Data
- Ads Campaign Performance Data
- SEO data from Google webmaster
- support Ticket Data
- A/B Test Data (Mobile, Web)
- Human curated data from Google Spreadsheets

	---

- 위 정보들을 ETL로 작성해서 Redshift로 복사함.

## C. ETL Pipelines

- All data pipelines were scheduled through Pinball
	- Every 5 minutes, hourly, daily, weekly and monthly
- Technology
	- Some pipelines are purely in Python
	- Some uses Hive/Spark for Batch Processing
	- Start using Kafka/SparkStreaming/Cassandra for Realtime Processing
- Purpose
	- Pure ETL (data transfer from sources to destination)^[순수 ETL (소스 간 데이터 전송)]
	- Data Transformation (summary info)^[데이터 변환(요약 정보)]
		- Some were copied back to the production database^[일부가 프로덕션 데이터페이스에 다시 복사되었다.]
	- A/B Test Analysis

	---

- 처음에는 pinball을 사용함 (cronjob 같은 스케줄러)
	- pinball도 python기반
- 외부나 내부에서 데이터를 복사해보는 ETL
- 이미 적재된 데이터를 summarize해주는 ETL
- A/B test 분석해주는 ETL
- 데이터를 프로세싱하여 반대로 production database로 복사해주는 ETL