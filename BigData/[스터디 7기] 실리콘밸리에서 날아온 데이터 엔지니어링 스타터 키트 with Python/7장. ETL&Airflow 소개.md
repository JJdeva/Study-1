# 0. 3주차 숙제 리뷰

## Assignment 1. 사용자별로 처음/마지막 채널 알아내기

```sql

WITH cte AS (
SELECT userid, channel, (ROW_NUMBER() OVER (PARTITION BY usc.userid ORDER BY  st.ts asc)) AS arn, (ROW_NUMBER() OVER (PARTITION BY usc.userid ORDER BY  st.ts desc)) AS drn
FROM raw_data.user_session_channel usc
JOIN raw_data.session_timestamp st ON usc.sessionid = st.sessionid
)

SELECT cte1.userid, cte1.channel AS first_touch, cte2.channel AS last_touch
FROM cte cte1
JOIN cte cte2 ON cte1.userid = cte2.userid 
WHERE cte1.arn = 1 and cte2.drn = 1
ORDER BY 1;

```

```sql

%%sql

SELECT DISTINCT usc.userid
 , FIRST_VALUE(usc.channel) OVER(PARTITION BY usc.userid ORDER BY st.ts ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
 , LAST_VALUE(usc.channel) OVER(PARTITION BY usc.userid ORDER BY st.ts ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)
FROM raw_data.user_session_channel usc
JOIN raw_data.session_timestamp st ON usc.sessionid = st.sessionid
ORDER BY 1
LIMIT 10
;

```

## Assignment 2. Gross Revenue가 가장 큰 UserID 10개 찾기

```sql

SELECT userID,
 SUM(amount) gross_revenue -- SUM(COALESCE(sum,0))?
 --, SUM(CASE WHEN refunded is FALSE THEN amount END) net_revenue
FROM raw_data.session_transaction st
JOIN raw_data.user_session_channel usc ON st.sessionid = usc.sessionid
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10;

```

## Assignment 3. 

생각을 제대로 안하고 INNER JOIN을 하면, 양쪽에서 공통된 필드만 넘어온다.

INNER JOIN은 교집합이기 때문.

```sql

SELECT LEFT(ts, 7) "month",
	usc.channel,
	COUNT(DISTINCT userid) uniqueUsers,
	COUNT(DISTINCT (CASE WHEN amount >= 0 THEN userid END)) paidUsers,
	ROUND(paidUsers::decimal*100/NULLIF(uniqueUsers, 0),2) conversionRate,
	SUM(amount) grossRevenue,
	SUM(CASE WHEN refunded is not True THEN amount END) netRevenue
FROM raw_data.user_session_channel usc
LEFT JOIN raw_data.session_timestamp t ON t.sessionid = usc.sessionid
LEFT JOIN raw_data.session_transaction st ON st.sessionid = usc.sessionid
GROUP BY 1, 2
ORDER BY 1, 2;

```

# 1. 데이터 파이프라인이란?

## A. 용어 설명

### a. ETL

- ETL (Extract, Transform and Load)
- Data Pipeline, ETL, Data Workflow, DAG, Data Job
	- ETL (Extract, Transform and Load)
	- Called DAG (Directed Acyclic Graph) in Airflow
- ETL vs ELT

---

- 데이터 파이프라인은 흔히 ETL이라고 부르는 것이다.
	- ETL은 Data Source에서 Data를 Extract(읽어서, 추출해서)하고 우리가 원하는 format으로 변환을 한 다음에 Data Warehouse에 적재하는 것
- 하나의 데이터 파이프라인 안에 여러개의 Task들이 있을 수 있음.
	- Task들이 그래프처럼 방향성을 가지고 Task가 끝나면 다음 Task가 실행되는 방식처럼 Task들의 실행 순서를 Graph처럼 표현할 수 있다.
- ELT
	- summary table 만드는 과정이 ELT
	- 이미 Data Warehouse에 raw data table들이 모여 있고 raw data table로부터 우리가 원하는 정보들을 뽑아서(join해서) 새로운 테이블을 만들어냄
	- Extract 대상이 대부분 이미 우리 안에 들어와 있는 데이터임.
		- Source가 DW일 수도 있고 DL이 될 수도 있음
		- 이미 우리 시스템(DW, DL)에 있는 데이터를 summary해서 원하는 정보를 만들어가지고 다시 DW에 적재하거나 Data Mart(특수한 형태의 DW)에 저장함.
	- DBT를 사용하기도 함
		- CTAS를 통해서 주기적으로 요약된 테이블을 만들어주는 것을 다양한 방법으로 툴로서 제공해주는 것이 DBT
	- ELT를 할 때는 SQL뿐만 아니라 Spark처럼 범용적인 대용량 프레임워크를 사용하기도 한다.
- ETL
	- Data Source들이 DW에 없고 외부에 있는 것
	- 외부에 있는 데이터를 읽어서 DW에 적재하는 것

보통 airflow 안에서 job들이 ETL 형태 혹은 ELT 형태로 돈다.

![](/bin/DE7_image/DE7_7_1.png)

### b. Data Lake VS Data Warehouse

다 비슷한데, 얼마나 더 clean한가, data의 크기가 얼마나 크냐에 따라서 Data Lake -> Data Warehouse -> Data Mart가 있다.

이런 늬앙스의 차이이지 이 세개가 아주 큰 차이가 있고 그런 것은 아님.

#### ㄱ) Data Lake

- Structured Data + Unstructured Data in various formats
- More like a historical data storage (no retention^[보유] policy)
- Order of magnitude bigger than Data Warehouse in terms of size

---

- **훨씬 더 sacable**한, storage가 훨씬 큰 곳
	- **다양한 data**
- **Structured Data 뿐만 아니라 Unstructured Data**도 있을 수 있음
	- Unstructured Data로는 log같은 것
- storage에 가까워서 data retention같은게 없음
	- 모든 historical data를 전부 저장함.
	- data retention으로 지난 1년치 데이터만 저장하는게 아니라, 법적인 이슈만 없으면 제약없이 모든 기간의 데이터를 저장
- AWS의 S3를 DL로 사용하는 회사도 있음.

#### ㄴ) Data Warehouse

- Focusing more on refined and structured data with some retention policy
- Usually BI tools (Looker, Tableau, Superset, ...) connect with Data Warehouse

---

- DL보다 조금 더 정제된 형태로 structured Data 형태만 저장
- data retention이 있음
	- 지난 15개월치만 저장해둔다거나.
	- 법적인 이슈가 있는 것들은 거기에 맞게 제약을 걸어둔다.
- DW는 대부분 관계형 데이터베이스이기 때문에 BI 툴을 통해 액세스하여 시각화하는 것은 보통 DW에 바로 연동한다.
- DW를 branch쳐서 재무팀을 위한 DW, 법무팀을 위한 DW 등으로 만들면 Data Mart라고 부르기도 함.

### c. Data Lake & ELT

![](/bin/DE7_image/DE7_7_2.png)

큰 회사나 이렇게 구성함.

작은 회사는 Data Lake를 구성할 필요까지는 없음.

회사가 커지면, 모든 데이터를 DW에 넣을 수도 없고 넣을 필요도 없음. 그래서 필요에 의해서 DL를 구성하기도 함.

### d. **Data Pipeline**의 정의

- **데이터를 소스로부터 목적지로 복사**하는 작업
	- 이 작업은 보통 코딩(파이썬 혹은 스칼라) 혹은 SQL을 통해 이뤄짐
	- 대부분의 경우 **목적지는 데이터 웨어하우스**가 됨
- 데이터 소스의 예
	- Click stream, call data, ads performance data, transactions, sensor data, metadata, ...
	- More concrete examples
		- production database, log files, API, stream data (Kafka topic)
- 데이터 목적지의 예
	- 데이터 웨어하우스, 캐시 시스템(Redis, Memcache), 프로덕션 데이터베이스, NoSQL, S3, ...

---

- 협의의 의미
	- ETL
	- 외부 data를 읽어서 DW에 적재하는 것
- 광의의 의미
	- summary table만드는 것처럼 source가 DW 자체가 되고 목적지도 DW가 된다.
	- source가 DW이고 목적지가 production db이나 외부 시스템일 수도 있다.

### e. Different Kinds of Data Pipelines

#### ㄱ) Raw Data ETL Jobs

1. 외부와 내부 데이터 소스에서 데이터를 읽어다가 (많은 경우 API를 통하게 됨)
2. 적장한 데이터 포맷 변환 후 (데이터의 크기가 커지면 Spark등이 필요해짐)
3. 데이터 웨어하우스 로드

이 작업은 보통 데이터 엔지니어가 함.

#### ㄴ) Summary/Report Jobs

1. DW(혹은 DL)로부터 데이터를 읽어 다시 DW에 쓰는 ETL
2. Raw Data를 읽어서 일종의 리포트 형태나 써머리 형태의 테이블을 다시 만드는 용도
3. 특수한 형태로는 AB테스트 결과를 분석하는 데이터 파이프라인도 존재

요약 테이블의 경우 SQL (CTAS를 통해)만으로 만들어지고 이는 데이터 분석가가 하는 것이 맞음. 데이터 엔지니어 관점에서는 어떻게 데이터 분석가들이 편하게 할 수 있는 환경을 만들어 주느냐가 관건

-> Analytics Engineer (DBT)

#### ㄷ) Production Data Jobs

1. DW로부터 데이터를 읽어 다른 Storage(많은 경우 프로덕션 환경)로 쓰는 ETL
	- 써머리 정보가 프로덕션 환경에서 성능 이유로 필요한 경우
	- 혹은 머신러닝 모델에서 필요한 피쳐들을 미리 계산해두는 경우
2. 이 경우 흔한 타겟 스토리지
	- Cassandra/HBase/DynamoDB와 같은 NoSQL
	- MySQL과 같은 관계형 데이터베이스 (OLTP)
	- Redis/Memcache와 같은 캐시
	- ElasticSearch와 같은 검색엔진

# 2. 간단한 ETL 작성해보기

## A. Extract, Transform, Load

- Extract
	- 데이터를 데이터 소스에서 읽어내는 과정
		```python
		def extract(url):
			# URL 읽어서 데이터 리턴
			return data
		```
- Transform
	- 필요하다면 그 원본 데이터의 포멧을 원하는 형태로 변경시키는 과정
	- 굳이 변환할 필요는 없다.
		```python
		def transform(data):
			# data를 name, gender
			# 리스트로 변환하여 리턴
			return list
		```
- Load
	- 최종적으로 Data Warehouse에 테이블로 집어넣는 과정
		```python
		def load(list):
			# list를 Redshift 테이블로 로드
		```

# 3. Airflow 소개

## A. Quick Airflow Intro

- Airflow is a platform for data pipelines in Python (beyond a scheduler)
	- Top level Apache project
- Data Pipeline Framework
	- To programmatically design, schedule & monitor data pipelines in Python3
- Supports data pipeline scheduling
	- Provides a web UI as well

---

- Airflow는 Python으로 만들어진 data pipeline을 위한 플랫폼
- 단순히 스케줄러가 아니라, 데이터 파이프라인의 작성을 쉽게 해주는 프레임워크이기도 함
- DAG를 만드는데 다양한 도움을 주는 프레임워크
- 스케줄링 지원
- data pipeline 간에 dependency를 걸 수 있음
- Web UI가 제공됨

---

- A data pipeline in Airflow is called "Directed Acyclic Graph" (DAG)
	- Composed of tasks (aka operators)
- Airflow is a cluster of one or more servers
	- Composed of worker(s) and a scheduler
	- The scheduler will distribute your tasks across a number of workers
	- DAG and scheduling info are stored in a DB (SQLite is used by default)
		- MySQL or Postgres is preferred for real production usage

---

- DAG
	- 하나의 data pipeline에는 다수의 task들의 집합
	- task들 간에 실행 순서가 정의 됨
	- 실행 순서는 Acyclic함
- Airflow 자체도 cluster임
	- DAG의 수가 늘어나고 DAG를 자주 실행해야 한다면 Airflow 한대로는 무리.
	- 그럼, Airflow에 server를 추가해야하고, 그러다보면 관리하는게 쉽지 않음
		- cluster로 돌아가는 것은 관리하기가 쉽지 않음
		- 돈이 있으면 cloud에 있는 것을 사용하는게 제일 좋음
			- google cloud compose, aws MWAA
	- 다양한 내부 정보들을 db에 저장해야함.
		- DAG들의 종류, 실행 시점, 과거 실행 기록 등 다양한 configuration 정보들
		- default 내부 DB로 SQLite를 사용함
			- SQLite로는 test도 못함
			- SQLite는 싱글 스레드 데이터베이스이고 파일 기반 데이터베이스라 production 또는 개발용도로도 못씀
		- 실습에서는 SQLite 대신에 postgreSQL을 사용함
			- postgreSQL을 airflow의 metadata를 저장하는 DB로 사용함.

---

- Very comprehensive integration with a lot of 3rd party services
	- [https://airflow.apache.org/docs/](https://airflow.apache.org/docs/)
- Airflow 2.0 was released in December 2020
	- We will use Airflow 2.0
	- [Airflow 1.x to 2.0 upgrade instructions](https://airflow.apache.org/docs/apache-airflow/stable/upgrading-to-2.html)

---

## B. Airflow - 총 5개의 컴포넌트로 구성

1. Web Server
	- Python Flask
2. Scheduler
	- cronjob
3. Worker
	- DAG를 실행해주는 worker
4. Database
	- SQLite가 기본으로 설치됨
5. Queue
	- 멀티노드 구성인 경우에만 사용됨
		- 멀티노드로 구성하게 되면 복잡도가 확 늘어서 cloud를 고려하는게 좋음
		- 서버가 다수 생긴다는 의미는 server failure가 발생할 확률이 늘어난다는 의미.
	- 이 경우 Executor가 달라짐 (CeleryExecuter, KubernetesExecutor)

## C. Airflow Architecture

### a. single node Airflow (node 하나짜리 airflow)

![](/bin/DE7_image/DE7_7_3.png)

노드 하나에 Webserver, Scheduler, Worker가 있음.

그리고 이 정보들이 meta store라는 관계형 데이터베이스에 저장됨

노드 1개일때도 queue가 있는데, queue라고 할 수도 없는 정도의 간단한 IPC(Inter-processor communication)을 queue 형태로 사용한다.

---

#### ㄱ) Scale Up

DAG가 증가하면 server의 사양을 올린다.

> 리소스가 부족해지면, 더 좋은 사양의 서버를 사용한다.

### b. multi node airflow

![](/bin/DE7_image/DE7_7_4.png)

Webserver와  scheduler는 master node에 있고 Worker의 수만 늘어난다.

worker가 master node와 같은 곳에 있는게 아니라 별개의 server로 존재한다.

scheduler가 task를 schedule할 때, 바로바로 worker한테 나눠주는게 아니라 queue에 넣으면 worker들이 읽어가는 형태로 바뀌게 된다.

#### ㄱ) Scale Out

scale 문제가 생기면 server를 늘려서 scaling 문제를 해결한다.

이 경우 scale ability가 더 좋긴 하지만, 유지보수 복잡도가 올라간다.

Scale out을 해야하는 시점에서는 cloud를 심각하게 고려해봐라.

## D. What is DAG?

- DAG
	- a set of tasks represented as a directed acyclic graph
- DAG is composed of Tasks
	- Tasks can be doing Extract, Transform and Load
- Task
	- an instance of an operator
		- e.g., Redshift writing, Postgres query, S3 Read/Write, Hive query, Spark job, shell script

![](/bin/DE7_image/DE7_7_5.png)

---

- DAG는 data pipeline이 해야하는 일들을 task들로 분리를 한 다음에 Task들 간에 실행 순서를 정한다.
	- **DAG는 task의 집합**이다.
		```ad-example

		이전의 실습에서 extract, transform, load가 각각 하나의 task

		```
- **task를 airflow에서는 operator**라고 부름.
	- operator마다 자기의 전문 분야가 있음
		```ad-example

		내가 하려는 일이 postgres db에 query를 날리는 것이면, 코딩을 다 하는 것이 아니라, postgres oprator를 가져다가 거기에 parameter를 바꾸면 끝나는 식으로 코딩이 간단해짐

		```
	- 가장 범용적으로 쓸 수 있는 것은 python operator임.
		- python operator로 만들어진 task는 내가 만든 python 코드가 실행될 때 실행된다.
	- Task들 간에 실행 순서가 define되어야 함.

## E. Pros and Cons of Airflow

### a. Pros

- Fine-grained DAG and dependency control
- Various operators and sensors detecting data availability
- Much easier backfill
- Programmable DAG framework with out box support of popular data sources

---

> 코드 하나를 가지고 backfill도 하고 오늘부터 미래의 데이터를 읽어오는데 쓸 수 있다.
> > 다만, 이렇게 되려면 airflow에서 원하는 형태로 코드를 작성해야 한다.

- airflow 프레임워크가 데이터 파이프라인을 만드는데 최적화된 것이기 때문에 다양한 형태로 실행 순서를 정의할 수 있다.
- 다양한 operator들을 통해 개발이 쉽다.
- backfill을 쉽게할 수 있다.
	- backfill
		- 이미 읽어온 데이터가 있는데, 그 데이터가 소스 쪽에서 바껴서 다시 다 읽어와야 하는 경우
		- 데이터를 복사하고, 과거 데이터를 읽어오는 것
		- 코드 하나를 가지고 오늘부터 미래까지 운영할 수 있고, 과거에 문제가 생겼을 때 코드 변경 없이 airflow UI에서 해결할 수 있다.

### b. Cons

- Relatively higher learning curve
- Long deployment cycle: developing, deploying, testing
	- Hard to test your codes to build DAGs on your laptop
- Maintenance can be hard (especially if you have a cluster of workers)
	- GCP provides "Cloud Composer" (2017~)
	- AWS provides "Managed Workflows for Apache Airflow" (2020~)

---

- 처음 배우는데 어렵다.
	- 용어가 헷갈림
- deploy나 test에 대해 잘 확립된 프로세스들이 있지는 않음
- multi node로 가는 순간 유지보수가 어려워진다.

## F. DAG Common Configuration Example

There are a lot more parameters to use!

```python

from datetime import datetime, timedelta
from airflow import DAG

default_args = {
	'owner': 'keeyong',
	'start_date': datetime(2020, 8, 7, hour=0, minute=00),
	'end_date': datetime(2020, 8, 31, hour=23, minute=00),
	'email': ['keeyonghan@hotmail.com'],
	'retries': 1,
	'retry_delay': timedelta(minutes=3),
}

```

- start_date and end_date specify when this dag starts and stops:
	- can be used to do one-off backfilling
	- Understanding implication of this is important along with **catchup**

---

`from airflow import DAG` 모듈이 항상 import가 되어야 한다.

DAG라는 class를 가지고 내가 구현하려고 하는 DAG에 대한 meta 정보(configuration)을 적어야 한다.

- configuration
	- owner
	- start date
		- 시작하는 날
		- DAG가 실행되는 날은 start date 다음 주기에 실행된다.
		- DAG가 실행되고 나서는 execute date = start date로 설정된다.
	- end date
		- 보통 안적음
			- 언제 끝나는지 모르면 적지 않음
		- 데이터 소스가 변경되어서 지난 1년치 데이터를 다시 읽어야하는 경우
	- email
		- DAG가 돌다가 fail이 발생했을 때 이메일 보낼 곳
	- retries
		- fail이 발생했을 때 retry 횟수
	- retry_delay
		- fail이 발생하고 얼마나 있다가 retry 할 것인지?

## G. DAG Object Creation Example

```python

test_dag = DAG(
	"dag_v1", # DAG name
	# schedule (same as cronjob)
	schedule_interval="0 9 * * *",
	# common settings
	default_args=default_args
)

```

- Schedule interval can be defined as cron expression or presets as follows:
	- None, @once, @hourly, @daily, @weekly, @monthly, @yearly
- Airflow (>1.8) supports UTC and local timezones

![](/bin/DE7_image/DE7_7_6.png)

---

 - DAG object를 만들 때 default_args parameter에 위에서 작성한 configuration을 지정한다.
 - DAG name
	 - Web UI에서 나타남
 - 스케줄링은 linux cron과 동일함

## H. Operators Creation Example

### a. #1

```python

t1 = BashOperator(
 task_id='print_date',
 bash_command='date',
 dag=test_dag)

t2 = BashOperator(
 task_id='sleep',
 bash_command='sleep 5',
 retries=3,
 dag=test_dag)

t3 = BashOperator(
 task_id='ls',
 bash_command='ls /tmp',
 dag=test_dag)

t1 >> t2 # t2.set_upstream(t1)
t1 >> t3 # t3.set_upstream(t1)

```

![](/bin/DE7_image/DE7_7_7.png)

---

- 실행 순서
	- t1이 끝나면 t2, t3를 동시에 실행해라

### b. #2

```python

start = DummyOperator(dag=dag, task_id="start", *args, **kwargs)

t1 = BashOperator(
 task_id='ls1',
 bash_command='ls /tmp/downloaded',
 retries=3,
 dag=dag)

t2 = BashOperator(
 task_id='ls2',
 bash_command='ls /tmp/downloaded',
 dag=dag)

end = DummyOperator(dag=dag, task_id='end', *args, **kwargs)

start >> t1 >> end
start >> t2 >> end 

# 위와 동일하게 동작함
# start >> [t1, t2] >> end

```

![](/bin/DE7_image/DE7_7_8.png)

---

- 실행 순서
	- start -> t1, t2 -> end

- DummyOperator
	- 아무것도 안하는 operator

# 4. Data pipeline을 만들 때 고려할 점

## A. 이상과 현실간의 괴리

- 이상 혹은 환상
	- 내가 만든 데이터 파이프라인은 문제 없이 동작할 것이다.
	- 내가 만든 데이터 파이프라인을 관리하는 것은 어렵지 않을 것이다.
- 현실 혹은 실상
	- 데이터 파이프라인은 많은 이유로 실패함
		- 버그
		- 데이터 소스상의 이유
			- What if data sources are not available or change its data format
		- 데이터 파이프라인들간의 의존도에 대한 이해도 부족
	- 데이터 파이프라인의 수가 늘어나면 유지보수 비용이 기하급수적으로 늘어남
		- 데이터 소스간의 의존도가 생기면서 이는 더 복잡해짐
			- 만일 마케팅 채널 정보가 업데이트가 안된다면 마케팅 관련 다른 모든 정보들이 갱신되지 않음
		- More tables needs to be managed (source of truth, search cost, ...)

---

- 데이터 파이프라인은 절대로 내가 원하는대로 동작하지 않는다.

- 결국 코딩이다.
	- 버그가 존재한다.
	- 실패할 것을 고려하고 돌려야한다.
- 정말 중요한 파이프라인이 무엇이고, 그 파이프라인들이 어떻게 동작하고, 소스에 문제가 생겼을 때는 어떻게 대응을 할 것이고 같은 부분이 잘 준비되어야 함.

## B. Best Practices

### a. 데이터를 읽어올 때

- 가능하면 데이터가 작을 경우 매번 통채로 복사해서 테이블을 만들기
- Incremental update만이 가능하다면, 대상 데이터소스가 갖춰야할 몇가지 조건이 있음
	- 데이터 소스가 프로덕션 데이터베이스 테이블이라면 다음 필드가 필요
		- created
		- modified
		- Deleted
	- 데이터 소스가 API라면 특정 날짜를 기준으로 새로 생성되거나 업데이트된 레코드들을 읽어올 수 있어야함

---

- 데이터를 읽어올 때, 제일 좋은건 매번 통채로 다 읽어오는 것임.
- 바뀐 것만 가져오자(Incremental update)를 꼭 해야하는 상황이 아니라면 매번 data source에 있는 것을 다 읽어다가 DW에 적재해야한다.

#### ㄱ) Incremental update를 해야하는 경우

- 데이터가 너무 커서 매번 통째로 읽어올 수 없는 경우
- 돈을 내고 부르는 API인데 API 호출 비용이 비싼 경우

#### ㄴ) Incremental update를 피해야하는 이유

- 복잡도를 증가시키기 때문.

#### ㄷ) Incremental update가 가능한 경우

- 데이터 소스가 몇가지 기능을 지원해줘야 함.
	- ~날짜 이후로 변경/삭제된 record들만 달라는 query를 할 수 있어야 함.
	- 특정 시간 / 특정 아이디 이후에 생기거나 변경된 record들만 달라는 query를 할 수 있어야 함.

#### ㄹ) Incremental update가 불가능한 경우

- Incremental update가 ideal한 경우에도 data source가 위 기능을 제공하지 않으면 Incremental update를 할 방법이 없다.

### b. 멱등성

- **멱등성(Idempotency)**을 보장하는 것이 중요
- 멱등성은 무엇인가?
	- 동일한 입력 데이터로 데이터 파이프라인을 다수 실행해도 최종 테이블의 내용이 달라지지 않아야함
	- 예를 들면 중복 데이터가 생기지 말아야함

---

- 동일한 입력이 있는 경우(입력 source가 바뀌지 않았을 때), 데이터 파이프라인을 몇 번 돌리던 DW에 있는 데이터는 동일해야한다.
	- data source가 달라지지 않았는데, 여러번 동작시킨다고 결과가 달라지면 안됨.

### c. Backfill

- 실패한 데이터 파이프라인을 재실행이 쉬워야함
- 과거 데이터를 다시 채우는 과정(Backfill)이 쉬워야함
- Airflow는 이 부분(특히 backfill)에 강점을 갖고 있음
	- DAG의 catchup 파라미터가 True가 되어야하고 start_date와 end_date가 적절하게 설정되어야함
	- 대상 테이블이 incremental update가 되는 경우에만 의미가 있음
		- execution_date 파라미터를 사용해서 업데이트되는 날짜 혹은 시간을 알아내게 코드를 작성해야함
			- 현재 시간을 기준으로 업데이트 대상을 선택하는 것은 안티 패턴

---

- 하나의 코드 베이스로 지금부터 미래의 데이터를 복사해오는 것을 지원하고, 필요한 경우 backfill을 지원하기도 함
	- 이게 되려면, Airflow 코드를 만들 때 Airflow에서 요구하는 형태로 코드를 작성해야 함.
- backfill은 Incremental update를 한다는 것을 의미한다.
- Airflow에서는 catchup, start_date, end_date parameter를 통해서 backfill을 결정함

### d. 문서화

- 데이터 파이프라인의 입력과 출력을 명확히 하고 문서화
	- 데이터 디스커버리 문제
- 주기적(quarter에 한번)으로 쓸모없는 데이터들을 삭제
	- Kill unused tables and data pipelines proactively
	- Retain only necessary data in DW and move past data to DL (or storage)

### e. 데이터 파이프라인 사고시 사고 리포트 작성

- 데이터 파이프라인 사고시마다 사고 리포트(post-mortem)쓰기
	- 목적은 동일한 혹은 아주 비슷한 사고가 또 발생하는 것을 막기 위함
- 중요 데이터 파이프라인의 입력과 출력을 체크하기
	- 아주 간단하게 입력 레코드 수와 출력 레코드의 수가 몇개인지 체크하는 것부터 시작
	- summary table을 만들어내고 Primary key가 존재한다면 Primary key uniqueness가 보장되는지 체크하는 것이 필요함
	- 중복 레코드 체크

# 5. Airflow의 Backfill 방식 설명

## A. Daily Incremental Update를 구현해야 한다면?

- 예를 들어 2020년 11월 7일날부터 매일매일 하루치 데이터를 읽어온다고 가정해보자
- 이 경우 언제부터 해당 ETL이 동작해야하나?
	- 2020년 11월 8일
- 2020년 11월 8일날 동작하지만 읽어와야 하는 데이터의 날짜는?
	- 2020년 11월 7일
	- Airflow의 start_date는 시작 날짜라기보다는 읽어와야하는 날짜임 (incremental update job인 경우)

## B. Incremental하게 1년치 데이터를 Backfill 해야한다면?

- 어떻게 ETL을 구현해놓으면 이런 일이 편해질까?

### a. 해결방법 1

- 기존 ETL 코드를 조금 수정해서 지난 1년치 데이터에 대해 돌린다.
- 실수하기 쉽고 수정하는데 시간이 걸림

### b. 해결방법 2

- 시스템적으로 이걸 쉽게 해주는 방법을 구현한다.
- 날짜를 지정해주면 그 기간에 대한 데이터를 다시 읽어온다.
	- Airflow의 접근방식
		- 모든 DAG 실행에는 "execution_date"이 지정되어 있음
		- execution_date으로 채워야하는 날짜와 시간이 넘어옴
		- 이를 바탕으로 데이터를 갱신하도록 코드를 작성해야함

## C. start_date와 execution_date 이해하기

- 2020-08-10 02:00:00로 start date가 설정된 daily job이 있다.
	- catchup이 True로 설정되어 있다고 가정 (디폴트가 True)
- 지금 시간이 2020-08-13 20:00:00이고 처음으로 이 job이 활성화되었다.

---

- catchup
	- start_date와 현재 시간을 봤을 때, start_date이 과거인 경우 그 사이에 밀린 task들을 어떻게 할지를 결정하는 parameter
	- 밀린 task들이 있다면 실행(catchup)해라.
	- catchup이 **Fasle**이면 과거는 신경쓰지 않고 미래부터 돈다.
- execution_date
	- airflow가 채워줌

### 이 경우 이 job은 몇 번 실행될까? (execution_date)

> 3번

#### 2020-08-11 02:00:00에

- 2020-08-10 02:00:00 ~ 2020-08-11 02:00:00이 실행됨
- start_date
	- 2020-08-10 02:00:00
- execution_date
	- 2020-08-10 02:00:00

#### 2020-08-12 02:00:00에

- 2020-08-11 02:00:00 ~ 2020-08-12 02:00:00이 실행됨
- start_date
	- 2020-08-11 02:00:00
- execution_date
	- 2020-08-11 02:00:00

#### 2020-08-13 02:00:00에

- 2020-08-12 02:00:00 ~ 2020-08-13 02:00:00이 실행됨
- start_date
	- 2020-08-12 02:00:00
- execution_date
	- 2020-08-12 02:00:00

