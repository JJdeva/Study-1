# 1. 1주차

## A. 수업 중 퀴즈

- 왜, skip scooters에서 활동적인 고객들이 2~3개월 후에 이탈하는가?
	- 직접 구매해서

## B. 1주차 퀴즈

- '데이터 웨어하우스' 라는 것은 반드시 빅데이터 기반의 데이터베이스를 말한다?
	- 네
	- **아니오**
- 이상적인 데이터 조직의 발전 단계의 첫 번째는 무엇일까요?
	- 데이터 과학자(DS)를 팀에 영입해, 모델링부터 하자!
	- 데이터 분석가(DA)를 팀에 영입해, metrics를 만들고 시각화부터 하자!
	- **데이터 엔지니어(DE)를 팀에 영입해, 데이터 인프라부터 만들자!**
- ETL을 만들면 이를 주기적으로 실행해주어야 합니다. 스케줄러로 가장 많이 사용되는 것은 무엇일까요?
	- 내 답변 **airflow**
- ETL은 무엇의 약자인가요? 또 어떤 것을 의미하나요? 생각을 정리할 겸 글로 적어봅시다!
	- 내 답변 **Extract / Transform / Load / production db의 data를 data warehouse에 적재하기 위해 data warehouse 형식에 맞춰 데이터를 변형하는 것.**
	- 광의
		- airflow 같은 스케줄러에서 돌아가는 모든 것
- 다음 중 많이 사용되는 데이터 웨어하우스 기술이 아닌 것은?
	- **오라클 데이터베이스**
	- AWS의 Redshift
	- 구글클라우드의 Big Query
	- Snowflake
- 요즘 데이터팀의 개발방식은?
	- 폭포수 (waterfall) 개발 방식
	- **애자일 (agile) 개발 방식**
- 현재 추세는 데이터 과학자도 코딩을 할 줄 알아야 한다?
	- **맞다**
	- 틀리다
- 다음 중 데이터 분석가가 하는 일이 아닌 것은?
	- 중요지표 정의
	- 시각화 대시보드 구현
	- **데이터 웨어하우스 구축**
- 데이터 과학자들이 꼭 기억해야하는 것이 아닌 것은?
	- 솔루션은 간단할수록 좋다
	- **머신러닝을 쓴다면 딥러닝이 가장 좋다**
	- 무슨 일을 하건 내 일의 성공여부를 결정해주는 지표를 생각해야 한다
- 데이터 일을 하는 사람들이 꼭 알아야할 기술을 하나 꼽는다면?
	- **SQL**
	- 파이썬
	- 머신러닝
- A/B 테스트가 무엇인지 간단한게 써보세요.
	- **현재 서비스중인 것과 새로 개발한 모델을 적용한 것을 사용자에게 랜덤하게 보여줘서, 원하는 부분(경험)이 개선되었는지를 비교하여 파악하는 방법**
- 이번 강의에서 배우고자 하는 것 다시 한번 써주세요!

# 3. 3주차

## A. 수업 중 퀴즈

- ![](/bin/DE7_image/de7_q_1.png)
	- 7 / 6 / 4

```sql

COUNT(1) == COUNT(*)

```

- Boolean 타입의 필드도 “IS TRUE” 혹은 “IS FALSE”로 비교
	- `AA Is Not False == AA Is True` the same?
		- **NO**
			- A: True, False, NULL -> WHERE A is True, WHERE A is not FALSE 

- NULL로 나누면? vs. 0으로 나누면?
	- **NULL / Error (divide by zero)**

# 4. 4주차

## A. 수업 중 퀴즈

- 2020-08-10 02:00:00로 start date가 설정된 daily job이 있다.
	- catchup이 True로 설정되어 있다고 가정 (디폴트가 True)
- 지금 시간이 2020-08-13 20:00:00이고 처음으로 이 job이 활성화되었다.

### 이 경우 이 job은 몇 번 실행될까? (execution_date)

> 3번

#### 2020-08-11 02:00:00에

- 2020-08-10 02:00:00 ~ 2020-08-11 02:00:00이 실행됨
- start_date
	- 2020-08-10 02:00:00
- execution_date
	- 2020-08-10 02:00:00

#### 2020-08-12 02:00:00에

- 2020-08-11 02:00:00 ~ 2020-08-12 02:00:00이 실행됨
- start_date
	- 2020-08-11 02:00:00
- execution_date
	- 2020-08-11 02:00:00

#### 2020-08-13 02:00:00에

- 2020-08-12 02:00:00 ~ 2020-08-13 02:00:00이 실행됨
- start_date
	- 2020-08-12 02:00:00
- execution_date
	- 2020-08-12 02:00:00

## B. 4주차 퀴즈

- 하나의 DAG는 다수의 ()로 구성된다. ()에 들어갈 말은?
	- **task**
- 매일 동작하는 DAG의 Start date이 2021-02-05라면 이 DAG의 첫 실행 날짜는?
	- 2021-02-05
	- **2021-02-06**
	- 2021-02-05 01:00:00
	- 2021-02-07
- 위 DAG의 경우 이때 execution_date으로 들어오는 날짜는?
	- **2021-02-05**
- Schedule interval이 "30 \* \* \* \*"으로 설정된 DAG에 대한 올바른 설명은?
	- 매일 0시 30분마다 한번씩 실행된다
	- **매시 30분마다 한번씩 실행된다**
	- 일요일마다 매시 30분에 한번씩 실행된다
- Schedule interval이 "0 \* \* \* \*"으로 설정된 DAG의 start date이 "2021-02-04 00:00:00"으로 잡혀있다면 이 DAG의 첫 번째 실행 날짜와 시간은 언제인가?
	- **2021-02-04 01:00:00**
- Airflow는 처음 ON이 되었을 때 start_date과 현재 날짜 사이에 실행이 안된 run들이 있을 경우 이를 실행한다. 이는 (??) 파라미터에 의해 결정된다. 이 파라미터를 False로 세팅하면 과거 실행이 안된 run을 무시한다
	- execution_date
	- dag
	- **catchup**
- 다음 중 Redshift에서 큰 데이터를 테이블로 복사하는 방식을 제대로 설명한 것은?
	- 하나씩 INSERT INTO를 실행하여 복사해준다
	- 복사할 레코드들을 파일로 저장해서 한번 Redshift로 올린다
	- **복사할 레코드들을 파일로 저장해서 S3로 올린 후에 거기서 Redshift로 벌크 복사한다**