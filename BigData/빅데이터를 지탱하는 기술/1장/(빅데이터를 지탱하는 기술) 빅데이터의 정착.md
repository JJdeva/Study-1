> 제 마음대로 요약하고 정리하는 것이라 빠진 내용들이 많습니다. 시간적 여유가 있으시면, 책을 구매하셔서 읽어보시는 것을 권유드립니다.

# 1. 빅데이터의 정착

"분산 시스템의 발전"과 "클라우드 서비스의 보급"에 따라 대량의 데이터를 효율적으로 처리하는 일이 점차 어렵게 되었다. 이 절에서는 "빅데이터"라는 단어가 퍼질 때까지의 역사를 되돌아본다.

![](/bin/Bigdata_image/bigdata_1_1.png)

```ad-note
title: 빅데이터의 주요 역사에 대해서 설명한다

- 2011년까지
	- Hadoop이나 NoSQL 데이터베이스 등 기반 기술의 발전
- 2012년까지
	- 클라우드 방식의 데이터 웨어하우스나 BI 도구의 보급
- 2013년부터
	- 스트림 처리나 애드 혹(AdHoc) 분석 환경의 확충

```

### A. 분산 시스템에 의한 데이터 처리의 고속화

> 빅데이터의 취급하기 어려운 점을 극복한 두 가지 대표 기술

현재로서는 '빅데이터의 기술이 큰 어려움 없이 안심하고 사용할 수 있다'고 말하기는 어려운 상황이며, 실제로 '데이터를 모아서 무엇을 할 것인가?'에 대해서도 아직은 명확하게 해답을 내리기 어려운 실정이다.

- 빅데이터의 취급이 어려운 이유
	- "데이터의 분석 방법을 모른다"는 점
	- "데이터 처리에 수고와 시간이 걸린다"는 점

이 책의 목적은 알고 싶은 정보가 이미 있다는 전제하에서 그것을 **어떻게 효율적으로 실행할 것인가?** 를 생각하는 것이다. 가능한 적은 노력으로 원하는 정보를 얻을 수 있도록 지금 어떠한 기술이 사용되는지 살펴보자.

![](/bin/Bigdata_image/bigdata_1_2.png)

##### a. 빅데이터 기술의 요구

> Hadoop과 NoSQL의 대두

빅데이터의 기술로 가장 먼저 예로 들 수 있는 것이 'Hadoop'과 'NoSQL'이다.

![](/bin/Bigdata_image/bigdata_1_0_1.png)

전통적인 관계형 데이터베이스(RDB)로는 취급할 수 없을 만큼 대량의 데이터가 점차 쌓이게 되었다. 그렇게 축적된 데이터를 처리하려면 기존과는 다른 구조가 필요했다. Hadoop과 NoSQL은 각각 다른 요구를 충족하기 위해 태어났다.

![](/bin/Bigdata_image/bigdata_1_3.png)

##### b. Hadoop

> 다수의 컴퓨터에서 대량의 데이터 처리

Hadoop은 **다수의 컴퓨터에서 대량의 데이터를 처리하기** 위한 시스템이다.

```ad-example
title: 예시

전 세계의 웹페이지를 모아서 검색 엔진을 만들려면 방대한 데이터를 저장해둘 스토리지와 데이터를 순차적으로 처리할 수 있는 구조가 필요하다. 그러기 위해서는 수백 대, 수천 대 단위의 컴퓨터가 이용되어야 하며, 그것을 관리하는 것이 Hadoop이라는 프레임워크다.

```

Hadoop은 구글에서 개발된 분산 처리 프레임워크인 **MapReduce**를 참고하여 제작되었다. 초기 Hadoop에서 MapReduce를 동작시키려면 데이터 처리의 내용을 기술하기 위해 자바 언어로 프로그래밍을 해야 했다. 그렇기에 누구나 간단히 사용하지 못했다.

그래서 SQL과 같은 쿼리 언어를 Hadoop에서 실행하기 위한 소프트웨어로 **Hive**가 개발되어 2009년에 출시되었다. Hive를 통해 프로그래밍 없이 데이터를 집계할 수 있게 함으로써 많은 사람이 Hadoop을 이용한 분산 시스템의 혜택을 받을 수 있게 되었다.

```ad-info
title: Hadoop의 중요 역사 (~2011년)

| 시기        | 이벤트                                                              |
| ----------- | ------------------------------------------------------------------- |
| 2004년 12월 | 구글에서 MapReduce 논문이 발표됨                                    |
| 2007년 9월  | Hadoop의 최초 버전(0.14.1)이 배포되어 전 세계적으로 이용되기 시작함 |
| 2009년 5월  | Hive의 최초 버전(0.3.0)이 배포됨                                    |
| 2011년 12월 | Hadoop 1.0.0 배포                                                                    |

```

##### c. NoSQL 데이터베이스

> 빈번한 읽기/쓰기 및 분산 처리가 강점

NoSQL은 전통적인 RDB의 제약을 제거하는 것을 목표로 한 데이터베이스의 총칭이다. NoSQL 데이터베이스에는 다양한 종류가 있다.

- 키 밸류 스토어 (key-value store/KVS)
	- 다수의 키와 값을 관련지어 저장
- 도큐멘트 스토어 (document store)
	- JSON과 같은 복잡한 데이터 구조를 저장
- 와이드 칼럼 스토어 (wide-column store)
	- 여러 키를 사용하여 높은 확장성을 제공

NoSQL 데이터베이스 제품은 각자가 추구하는 목표가 다르므로 단순 비교를 할 수는 없지만, RDB보다 고속의 읽기, 쓰기가 가능하고 분산 처리에 뛰어나다.

모여진 데이터를 나중에 집계하는 것이 목적인 Hadoop과 다르게 NoSQL은 애플리케이션에서 온라인으로 접속하는 데이터베이스다.

```ad-info
title: 주요 NoSQL 데이터베이스의 역사 (~2011년)

| 시기        | 이벤트             | 제품의 종류        |
| ----------- | ------------------ | ------------------ |
| 2009년 8월  | MongoDB 1.0 배포   | 도큐먼트 스토어    |
| 2010년 7월  | CouchDB 1.0 배포   | 도큐먼트 스토어    |
| 2011년 9월  | Riak 1.0 배포      | 키밸류 스토어      |
| 2011년 10월 | Cassandra 1.0 배포 | 와이드 칼럼 스토어 |
| 2011년 12월 | Redis 1.0 배포     | 키밸류 스토어      |

```

##### d. Hadoop과 NoSQL 데이터베이스의 조합

> 현실적인 비용으로 대규모 데이터 처리 실현

이 둘을 조합함으로써 **NoSQL 데이터베이스에 기록하고 Hadoop으로 분산 처리하기**라는 흐름이 2011년 말까지 정착하게 되었고, 2012년부터는 일반에 널리 퍼지기 시작했다.

방대한 규모로 계속 증가하는 데이터에 대해, 기존의 기술로는 불가능하거나 고가의 하드웨어가 필요한 경우에도 **현실적인 비용으로 데이터를 처리할 수 있게 된** 것이 당시의 기술적인 배경이다.

### B. 분산 시스템의 비즈니스 이용 개척

> 데이터 웨어하우스와의 공존

입루 기업에서는 이전부터 데이터 분석을 기반으로 하는 **엔터프라이즈 데이터 웨어하우스(enterprise data warehouse/EDW 또는 데이터 웨어하우스/DWH)** 를 도입했다.

분산 시스템의 발전에 따라, 기존이라면 데이터 웨어하우스 제품이 사용되는 경우에도 Hadoop을 사용하는 경우가 증가했다. 다수의 데이터 분석 도구가 Hadoop에 대한 대응을 표명하여 대량의 데이터를 보존 및 집계하기 위해 Hadoop과 Hive를 사용하게 되었다. 그 결과 Hadoop의 도입을 기술적으로 지원하는 비즈니스가 성립하게 되었다. 그리고 그때 사용하게 된 키워드가 **빅데이터**다.

데이터 웨어하우스에서도 대량의 데이터를 처리할 수 있으며, 오히려 여러 방면에서 Hadoop보다도 우수하다. 하지만 일부 데이터 웨어하우스 제품은 안정적인 성능을 실현하기 위해 하드웨어와 소프트웨어가 통합된 통합 장비(appliance)로 제공되었다. 데이터 용량을 늘리려면 하드웨어를 교체해야 하는 등 나중에 확장하기가 쉽지 않았다.

따라서, 가속도적으로 늘어나는 데이터의 처리는 Hadoop에 맡기고, 비교적 작은 데이터, 또는 중요한 데이터만을 데이터 웨어하우스에 넣는 식으로 사용을 구분하게 되었다.

```ad-example
title: 예시

야간 배치 등 심야에 대량으로 발생하는 데이터 처리에 Hadoop을 사용하고 있는데, 야간 배치에서는 매일 거래되는 데이터 등을 심야에 집계하여 다음 날 아침까지 보고서에 정리한다. 데이터양이 증가하면 배치 처리 또한 시간이 걸려 보고서의 완성이 늦어지고 이로 인해 업무에 지장이 생긴다. 그런 이유로 확장성이 뛰어난 Hadoop에 데이터 처리를 맡김으로써 데이터 웨어하우스의 부하를 줄이고 있다.

```

![](/bin/Bigdata_image/bigdata_1_0_2.png)

![](/bin/Bigdata_image/bigdata_1_4.png)

### C. 직접 할 수 있는 데이터 분석 폭 확대

> 클라우드 서비스와 데이터 디스커버리로 가속하는 빅데이터의 활용

비슷한 시기부터 클라우드 서비스의 보급에 의해 빅데이터의 활용이 증가하였다. **여러 컴퓨터에 분산 처리한다**라는 점이 빅데이터의 특징이다. 하지만 이를 위한 하드웨어를 준비하고 관리하는 일은 간단하지 않다. 클라우드 시대인 요즘은 시간 단위로 필요한 자원을 확보할 수 있어서 방법만 알면 언제든지 이용할 수 있는 환경이 마련되었다.

```ad-info
title: 데이터 처리를 위한 클라우드 서비스

| 시기        | 이벤트                        | 서비스의 특징          |
| ----------- | ----------------------------- | ---------------------- |
| 2009년 4월  | Amazon Elastic MapReduce 발표 | 클라우드를 위한 Hadoop |
| 2010년 5월  | 구글 BigQuery 발표            | 데이터 웨어하우스      |
| 2012년 10월 | Azure HDInsight 발표          | 클라우드를 위한 Hadoop |
| 2012년 11월 | Amazon Redshift 발표          | 데이터 웨어하우스      |

```

##### a. 데이터 디스커버리의 기초지식

> 셀프서비스용 BI 도구

빅데이터 기술이 나오기 시작한 시기와 같은 시기에 데이터 웨어하우스에 저장된 데이터를 시각화하려는 방법으로 **데이터 디스커버리(data discovery)**  가 인기를 끌게 되었다.

데이터 디스커버리란 "대화형으로 데이터를 시각화하여 가치 있는 정보를 찾으려고 하는 프로세스"를 말한다.

데이터 디스커버리는 "셀프서비스용 BI 도구"로 불린다. **BI 도구(business intelligence tool)** 는 예전부터 데이터 웨어하우스와 조합되어 사용된 경영자용 시각화 시스템으로 대기업의 IT부서에 의해 도입되는 대규모의 도구다. 셀프서비스용 BI 도구는 이것을 개인도 도입할 수 있을 정도로 단순화한 것으로, 이로 이해 점차 많은 사람이 데이터를 살펴볼 수 있게 되었다.

2013년 이후에도 빅데이터 기술은 더 높은 **효율**과 **편리성**을 실현하기 위해 계속해서 개발되고 있다. **Apache Spark(아파치 스파크)** 와 같은 새로운 분산 시스템용 프레임워크가 보급됨으로써 MapReduce보다도 효율적으로 데이터 처리를 할 수 있게 되었다. 배치 처리뿐만 아니라 실시간 데이터 처리를 위한 시스템도 다수 만들어지고 있다.

![](/bin/Bigdata_image/bigdata_1_5.png)

### D. 스몰 데이터와 빅데이터의 활용

> 스몰 데이터의 기술도 중요

기존 기술을 이용해서 취급할 수 있는 작은 데이터를 **스몰 데이터(small data)** 라고 한다.

```ad-example
title: 예시

한 대의 노트북에서 큰 부담 없이 처리할 수 만큼의 작은 데이터라 할 수 있다.
레코드 수로는 약 수백만에서 수천만 건, 데이터양으로 따지자면 '수 GB(giga byte)'까지를 스몰 데이터라고 한다.

```

빅데이터와 스몰 데이터 모두 그냥 데이터이며, 이 점에 있어서 본질적인 차이는 없다. 대량의 데이터인 경우, 과거라면 버릴 수밖에 없었던 데이터였지만,  빅데이터의 시대가 되니 그것마저도 모두 처리할 수 있게 되었다. 데이터 분석 방법은 스몰 데이터 시절부터 이미 존재하고 있었으므로 결국은 **효율**의 문제다.

사내에서 작성한 Excel 파일, 웹 에서 다운로드한 CSV 파일 등 이 세상은 대량의 스몰 데이터로 가득 차 있다. 효율적인 스몰 데이터의 처리 방법을 알지 못한 채, 빅데이터 기술만 배워서는 충분하지 않다. 이 둘을 모두 적재적소로 구사하는 것이 이상적이다.

![](/bin/Bigdata_image/bigdata_1_0_3.png)

# 참고문헌

[1] 니시다 케이스케, 장성두 옮김, "빅데이터를 지탱하는 기술", 3쇄, 제이펍, 2021년


